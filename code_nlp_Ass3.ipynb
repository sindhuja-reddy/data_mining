{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Programming Assignment 3"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Character-level Language Modeling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('shakespeare.txt', 'r') as f:\n",
    "    text = f.read()\n",
    "\n",
    "text = text.lower()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "Tx = 40\n",
    "stride = 3\n",
    "inputs = []\n",
    "targets = []\n",
    "for i in range(0, len(text) - Tx, stride):\n",
    "    inputs.append(text[i:i+Tx])\n",
    "    targets.append(text[i+1:i+Tx+1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "chars = set(text)\n",
    "vocab_size = len(chars)\n",
    "char_to_ix = {ch: i for i, ch in enumerate(sorted(chars))}\n",
    "ix_to_char = {i: ch for i, ch in enumerate(sorted(chars))}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def to_one_hot(char):\n",
    "    one_hot = np.zeros((vocab_size,))\n",
    "    one_hot[char_to_ix[char]] = 1\n",
    "    return one_hot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of train samples: 31213\n",
      "Size of the vocabulary: 38\n"
     ]
    }
   ],
   "source": [
    "X = np.zeros((len(inputs), Tx, vocab_size))\n",
    "Y = np.zeros((len(inputs), Tx, vocab_size))\n",
    "for i, (input_seq, target_seq) in enumerate(zip(inputs, targets)):\n",
    "    for j, char in enumerate(input_seq):\n",
    "        X[i, j, :] = to_one_hot(char)\n",
    "    for j, char in enumerate(target_seq):\n",
    "        Y[i, j, :] = to_one_hot(char)\n",
    "\n",
    "print(f\"Number of train samples: {X.shape[0]}\")\n",
    "print(f\"Size of the vocabulary: {vocab_size}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[[0., 0., 0., ..., 0., 0., 0.],\n",
       "        [0., 0., 0., ..., 0., 0., 0.],\n",
       "        [0., 0., 0., ..., 0., 0., 0.],\n",
       "        ...,\n",
       "        [0., 0., 0., ..., 0., 0., 0.],\n",
       "        [0., 0., 0., ..., 0., 0., 0.],\n",
       "        [0., 0., 0., ..., 0., 0., 0.]],\n",
       "\n",
       "       [[0., 1., 0., ..., 0., 0., 0.],\n",
       "        [0., 0., 0., ..., 0., 0., 0.],\n",
       "        [0., 0., 0., ..., 0., 0., 0.],\n",
       "        ...,\n",
       "        [0., 0., 0., ..., 0., 0., 0.],\n",
       "        [0., 1., 0., ..., 0., 0., 0.],\n",
       "        [0., 0., 0., ..., 0., 0., 0.]],\n",
       "\n",
       "       [[0., 0., 0., ..., 0., 0., 0.],\n",
       "        [0., 0., 0., ..., 0., 0., 0.],\n",
       "        [0., 0., 0., ..., 0., 0., 0.],\n",
       "        ...,\n",
       "        [0., 0., 0., ..., 0., 0., 0.],\n",
       "        [0., 0., 0., ..., 0., 0., 0.],\n",
       "        [0., 0., 0., ..., 0., 0., 0.]],\n",
       "\n",
       "       ...,\n",
       "\n",
       "       [[0., 0., 0., ..., 0., 0., 0.],\n",
       "        [0., 1., 0., ..., 0., 0., 0.],\n",
       "        [0., 0., 0., ..., 0., 0., 0.],\n",
       "        ...,\n",
       "        [0., 1., 0., ..., 0., 0., 0.],\n",
       "        [0., 1., 0., ..., 0., 0., 0.],\n",
       "        [0., 1., 0., ..., 0., 0., 0.]],\n",
       "\n",
       "       [[0., 0., 0., ..., 0., 1., 0.],\n",
       "        [0., 1., 0., ..., 0., 0., 0.],\n",
       "        [0., 0., 0., ..., 0., 0., 0.],\n",
       "        ...,\n",
       "        [0., 0., 0., ..., 0., 0., 0.],\n",
       "        [0., 0., 0., ..., 0., 0., 0.],\n",
       "        [0., 0., 0., ..., 0., 0., 0.]],\n",
       "\n",
       "       [[0., 0., 0., ..., 0., 0., 0.],\n",
       "        [0., 0., 0., ..., 0., 0., 0.],\n",
       "        [0., 0., 0., ..., 0., 0., 0.],\n",
       "        ...,\n",
       "        [0., 1., 0., ..., 0., 0., 0.],\n",
       "        [0., 0., 0., ..., 0., 0., 0.],\n",
       "        [0., 0., 0., ..., 0., 0., 0.]]])"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[[0., 0., 0., ..., 0., 0., 0.],\n",
       "        [0., 0., 0., ..., 0., 0., 0.],\n",
       "        [0., 1., 0., ..., 0., 0., 0.],\n",
       "        ...,\n",
       "        [0., 0., 0., ..., 0., 0., 0.],\n",
       "        [0., 0., 0., ..., 0., 0., 0.],\n",
       "        [0., 0., 0., ..., 0., 0., 0.]],\n",
       "\n",
       "       [[0., 0., 0., ..., 0., 0., 0.],\n",
       "        [0., 0., 0., ..., 0., 0., 0.],\n",
       "        [0., 0., 0., ..., 0., 0., 0.],\n",
       "        ...,\n",
       "        [0., 1., 0., ..., 0., 0., 0.],\n",
       "        [0., 0., 0., ..., 0., 0., 0.],\n",
       "        [0., 0., 0., ..., 0., 0., 0.]],\n",
       "\n",
       "       [[0., 0., 0., ..., 0., 0., 0.],\n",
       "        [0., 0., 0., ..., 0., 0., 0.],\n",
       "        [0., 0., 0., ..., 0., 0., 0.],\n",
       "        ...,\n",
       "        [0., 0., 0., ..., 0., 0., 0.],\n",
       "        [0., 0., 0., ..., 0., 0., 0.],\n",
       "        [0., 0., 0., ..., 0., 0., 0.]],\n",
       "\n",
       "       ...,\n",
       "\n",
       "       [[0., 1., 0., ..., 0., 0., 0.],\n",
       "        [0., 0., 0., ..., 0., 0., 0.],\n",
       "        [0., 0., 0., ..., 0., 1., 0.],\n",
       "        ...,\n",
       "        [0., 1., 0., ..., 0., 0., 0.],\n",
       "        [0., 1., 0., ..., 0., 0., 0.],\n",
       "        [0., 0., 0., ..., 0., 0., 0.]],\n",
       "\n",
       "       [[0., 1., 0., ..., 0., 0., 0.],\n",
       "        [0., 0., 0., ..., 0., 0., 0.],\n",
       "        [0., 0., 0., ..., 0., 0., 0.],\n",
       "        ...,\n",
       "        [0., 0., 0., ..., 0., 0., 0.],\n",
       "        [0., 0., 0., ..., 0., 0., 0.],\n",
       "        [0., 1., 0., ..., 0., 0., 0.]],\n",
       "\n",
       "       [[0., 0., 0., ..., 0., 0., 0.],\n",
       "        [0., 0., 0., ..., 0., 0., 0.],\n",
       "        [0., 0., 0., ..., 0., 0., 0.],\n",
       "        ...,\n",
       "        [0., 0., 0., ..., 0., 0., 0.],\n",
       "        [0., 0., 0., ..., 0., 0., 0.],\n",
       "        [0., 0., 0., ..., 0., 0., 0.]]])"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "31213\n",
      "31213\n"
     ]
    }
   ],
   "source": [
    "print(len(X))\n",
    "print(len(Y))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/1000\n",
      "244/244 [==============================] - 142s 548ms/step - loss: 2.7211 - accuracy: 0.2342\n",
      "Epoch 2/1000\n",
      "244/244 [==============================] - 140s 574ms/step - loss: 1.8633 - accuracy: 0.4415\n",
      "Epoch 3/1000\n",
      "244/244 [==============================] - 143s 588ms/step - loss: 1.7125 - accuracy: 0.4795\n",
      "Epoch 4/1000\n",
      "244/244 [==============================] - 144s 588ms/step - loss: 1.6413 - accuracy: 0.4974\n",
      "Epoch 5/1000\n",
      "244/244 [==============================] - 147s 602ms/step - loss: 1.5961 - accuracy: 0.5083\n",
      "Epoch 6/1000\n",
      "244/244 [==============================] - 142s 580ms/step - loss: 1.5636 - accuracy: 0.5171\n",
      "Epoch 7/1000\n",
      "244/244 [==============================] - 142s 580ms/step - loss: 1.5401 - accuracy: 0.5223\n",
      "Epoch 8/1000\n",
      "244/244 [==============================] - 145s 596ms/step - loss: 1.5216 - accuracy: 0.5269\n",
      "Epoch 9/1000\n",
      "244/244 [==============================] - 144s 589ms/step - loss: 1.5069 - accuracy: 0.5315\n",
      "Epoch 10/1000\n",
      "244/244 [==============================] - 143s 588ms/step - loss: 1.4942 - accuracy: 0.5338\n",
      "Epoch 11/1000\n",
      "244/244 [==============================] - 143s 587ms/step - loss: 1.4821 - accuracy: 0.5376\n",
      "Epoch 12/1000\n",
      "244/244 [==============================] - 142s 583ms/step - loss: 1.4754 - accuracy: 0.5389\n",
      "Epoch 13/1000\n",
      "244/244 [==============================] - 139s 568ms/step - loss: 1.4638 - accuracy: 0.5418\n",
      "Epoch 14/1000\n",
      "244/244 [==============================] - 143s 586ms/step - loss: 1.4548 - accuracy: 0.5445\n",
      "Epoch 15/1000\n",
      "244/244 [==============================] - 140s 574ms/step - loss: 1.4515 - accuracy: 0.5456\n",
      "Epoch 16/1000\n",
      "244/244 [==============================] - 138s 565ms/step - loss: 1.4419 - accuracy: 0.5475\n",
      "Epoch 17/1000\n",
      "244/244 [==============================] - 144s 589ms/step - loss: 1.4376 - accuracy: 0.5487\n",
      "Epoch 18/1000\n",
      "244/244 [==============================] - 355s 1s/step - loss: 1.4319 - accuracy: 0.5504\n",
      "Epoch 19/1000\n",
      "244/244 [==============================] - 147s 602ms/step - loss: 1.4248 - accuracy: 0.5524\n",
      "Epoch 20/1000\n",
      "244/244 [==============================] - 142s 581ms/step - loss: 1.4244 - accuracy: 0.5528\n",
      "Epoch 21/1000\n",
      "244/244 [==============================] - 150s 614ms/step - loss: 1.4170 - accuracy: 0.5547\n",
      "Epoch 22/1000\n",
      "244/244 [==============================] - 156s 640ms/step - loss: 1.4160 - accuracy: 0.5542\n",
      "Epoch 23/1000\n",
      "244/244 [==============================] - 141s 580ms/step - loss: 1.4109 - accuracy: 0.5561\n",
      "Epoch 24/1000\n",
      "244/244 [==============================] - 146s 597ms/step - loss: 1.4090 - accuracy: 0.5571\n",
      "Epoch 25/1000\n",
      "244/244 [==============================] - 143s 584ms/step - loss: 1.4053 - accuracy: 0.5577\n",
      "Epoch 26/1000\n",
      "244/244 [==============================] - 143s 586ms/step - loss: 1.4002 - accuracy: 0.5594\n",
      "Epoch 27/1000\n",
      "244/244 [==============================] - 144s 588ms/step - loss: 1.3977 - accuracy: 0.5597\n",
      "Epoch 28/1000\n",
      "244/244 [==============================] - 145s 595ms/step - loss: 1.3957 - accuracy: 0.5602\n",
      "Epoch 29/1000\n",
      "244/244 [==============================] - 144s 589ms/step - loss: 1.3886 - accuracy: 0.5621\n",
      "Epoch 30/1000\n",
      "244/244 [==============================] - 144s 591ms/step - loss: 1.3907 - accuracy: 0.5614\n",
      "Epoch 31/1000\n",
      "244/244 [==============================] - 144s 590ms/step - loss: 1.3865 - accuracy: 0.5627\n",
      "Epoch 32/1000\n",
      "244/244 [==============================] - 144s 590ms/step - loss: 1.3829 - accuracy: 0.5641\n",
      "Epoch 33/1000\n",
      "244/244 [==============================] - 144s 590ms/step - loss: 1.3831 - accuracy: 0.5635\n",
      "Epoch 34/1000\n",
      "244/244 [==============================] - 144s 589ms/step - loss: 1.3782 - accuracy: 0.5653\n",
      "Epoch 35/1000\n",
      "244/244 [==============================] - 143s 584ms/step - loss: 1.3767 - accuracy: 0.5656\n",
      "Epoch 36/1000\n",
      "244/244 [==============================] - 141s 578ms/step - loss: 1.3767 - accuracy: 0.5657\n",
      "Epoch 37/1000\n",
      "244/244 [==============================] - 140s 573ms/step - loss: 1.3711 - accuracy: 0.5668\n",
      "Epoch 38/1000\n",
      "244/244 [==============================] - 139s 570ms/step - loss: 1.3725 - accuracy: 0.5669\n",
      "Epoch 39/1000\n",
      "244/244 [==============================] - 139s 570ms/step - loss: 1.3698 - accuracy: 0.5677\n",
      "Epoch 40/1000\n",
      "244/244 [==============================] - 140s 572ms/step - loss: 1.3674 - accuracy: 0.5680\n",
      "Epoch 41/1000\n",
      "244/244 [==============================] - 140s 573ms/step - loss: 1.3653 - accuracy: 0.5684\n",
      "Epoch 42/1000\n",
      "244/244 [==============================] - 139s 570ms/step - loss: 1.3643 - accuracy: 0.5686\n",
      "Epoch 43/1000\n",
      "244/244 [==============================] - 138s 567ms/step - loss: 1.3616 - accuracy: 0.5697\n",
      "Epoch 44/1000\n",
      "244/244 [==============================] - 139s 571ms/step - loss: 1.3603 - accuracy: 0.5701\n",
      "Epoch 45/1000\n",
      "244/244 [==============================] - 144s 592ms/step - loss: 1.3616 - accuracy: 0.5693\n",
      "Epoch 46/1000\n",
      "244/244 [==============================] - 142s 584ms/step - loss: 1.3578 - accuracy: 0.5715\n",
      "Epoch 47/1000\n",
      "244/244 [==============================] - 143s 585ms/step - loss: 1.3578 - accuracy: 0.5715\n",
      "Epoch 48/1000\n",
      "244/244 [==============================] - 143s 587ms/step - loss: 1.3540 - accuracy: 0.5725\n",
      "Epoch 49/1000\n",
      "244/244 [==============================] - 143s 586ms/step - loss: 1.3531 - accuracy: 0.5726\n",
      "Epoch 50/1000\n",
      "244/244 [==============================] - 142s 584ms/step - loss: 1.3508 - accuracy: 0.5729\n",
      "Epoch 51/1000\n",
      "244/244 [==============================] - 168s 688ms/step - loss: 1.3494 - accuracy: 0.5738\n",
      "Epoch 52/1000\n",
      "244/244 [==============================] - 163s 670ms/step - loss: 1.3479 - accuracy: 0.5740\n",
      "Epoch 53/1000\n",
      "244/244 [==============================] - 179s 732ms/step - loss: 1.3455 - accuracy: 0.5750\n",
      "Epoch 54/1000\n",
      "244/244 [==============================] - 139s 568ms/step - loss: 1.3481 - accuracy: 0.5745\n",
      "Epoch 55/1000\n",
      "244/244 [==============================] - 139s 571ms/step - loss: 1.3463 - accuracy: 0.5742\n",
      "Epoch 56/1000\n",
      "244/244 [==============================] - 165s 675ms/step - loss: 1.3432 - accuracy: 0.5753\n",
      "Epoch 57/1000\n",
      "244/244 [==============================] - 184s 756ms/step - loss: 1.3465 - accuracy: 0.5741\n",
      "Epoch 58/1000\n",
      "244/244 [==============================] - 192s 788ms/step - loss: 1.3453 - accuracy: 0.5745\n",
      "Epoch 59/1000\n",
      "244/244 [==============================] - 148s 606ms/step - loss: 1.3395 - accuracy: 0.5762\n",
      "Epoch 60/1000\n",
      "244/244 [==============================] - 139s 571ms/step - loss: 1.3363 - accuracy: 0.5775\n",
      "Epoch 61/1000\n",
      "244/244 [==============================] - 139s 569ms/step - loss: 1.3387 - accuracy: 0.5762\n",
      "Epoch 62/1000\n",
      "244/244 [==============================] - 138s 566ms/step - loss: 1.3365 - accuracy: 0.5770\n",
      "Epoch 63/1000\n",
      "244/244 [==============================] - 139s 571ms/step - loss: 1.3366 - accuracy: 0.5772\n",
      "Epoch 64/1000\n",
      "244/244 [==============================] - 138s 566ms/step - loss: 1.3346 - accuracy: 0.5773\n",
      "Epoch 65/1000\n",
      "244/244 [==============================] - 144s 590ms/step - loss: 1.3325 - accuracy: 0.5779\n",
      "Epoch 66/1000\n",
      "244/244 [==============================] - 143s 585ms/step - loss: 1.3342 - accuracy: 0.5775\n",
      "Epoch 67/1000\n",
      "244/244 [==============================] - 143s 586ms/step - loss: 1.3290 - accuracy: 0.5796\n",
      "Epoch 68/1000\n",
      "244/244 [==============================] - 143s 586ms/step - loss: 1.3324 - accuracy: 0.5789\n",
      "Epoch 69/1000\n",
      "244/244 [==============================] - 144s 590ms/step - loss: 1.3292 - accuracy: 0.5791\n",
      "Epoch 70/1000\n",
      "244/244 [==============================] - 143s 587ms/step - loss: 1.3283 - accuracy: 0.5795\n",
      "Epoch 71/1000\n",
      "244/244 [==============================] - 142s 583ms/step - loss: 1.3271 - accuracy: 0.5797\n",
      "Epoch 72/1000\n",
      "244/244 [==============================] - 142s 584ms/step - loss: 1.3269 - accuracy: 0.5792\n",
      "Epoch 73/1000\n",
      "244/244 [==============================] - 146s 600ms/step - loss: 1.3239 - accuracy: 0.5807\n",
      "Epoch 74/1000\n",
      "244/244 [==============================] - 144s 592ms/step - loss: 1.3255 - accuracy: 0.5802\n",
      "Epoch 75/1000\n",
      "244/244 [==============================] - 143s 585ms/step - loss: 1.3248 - accuracy: 0.5799\n",
      "Epoch 76/1000\n",
      "244/244 [==============================] - 143s 587ms/step - loss: 1.3229 - accuracy: 0.5811\n",
      "Epoch 77/1000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "244/244 [==============================] - 142s 583ms/step - loss: 1.3229 - accuracy: 0.5806\n",
      "Epoch 78/1000\n",
      "244/244 [==============================] - 143s 588ms/step - loss: 1.3215 - accuracy: 0.5821\n",
      "Epoch 79/1000\n",
      "244/244 [==============================] - 142s 582ms/step - loss: 1.3209 - accuracy: 0.5817\n",
      "Epoch 80/1000\n",
      "244/244 [==============================] - 143s 587ms/step - loss: 1.3194 - accuracy: 0.5819\n",
      "Epoch 81/1000\n",
      "244/244 [==============================] - 145s 592ms/step - loss: 1.3197 - accuracy: 0.5824\n",
      "Epoch 82/1000\n",
      "244/244 [==============================] - 142s 582ms/step - loss: 1.3182 - accuracy: 0.5826\n",
      "Epoch 83/1000\n",
      "244/244 [==============================] - 143s 587ms/step - loss: 1.3180 - accuracy: 0.5825\n",
      "Epoch 84/1000\n",
      "244/244 [==============================] - 142s 583ms/step - loss: 1.3196 - accuracy: 0.5818\n",
      "Epoch 85/1000\n",
      "244/244 [==============================] - 42069s 173s/step - loss: 1.3160 - accuracy: 0.5832\n",
      "Epoch 86/1000\n",
      "244/244 [==============================] - 158s 647ms/step - loss: 1.3154 - accuracy: 0.5830\n",
      "Epoch 87/1000\n",
      "244/244 [==============================] - 142s 582ms/step - loss: 1.3170 - accuracy: 0.5831\n",
      "Epoch 88/1000\n",
      "244/244 [==============================] - 140s 572ms/step - loss: 1.3162 - accuracy: 0.5828\n",
      "Epoch 89/1000\n",
      "244/244 [==============================] - 140s 573ms/step - loss: 1.3135 - accuracy: 0.5845\n",
      "Epoch 90/1000\n",
      "244/244 [==============================] - 141s 577ms/step - loss: 1.3131 - accuracy: 0.5836\n",
      "Epoch 91/1000\n",
      "244/244 [==============================] - 148s 605ms/step - loss: 1.3128 - accuracy: 0.5839\n",
      "Epoch 92/1000\n",
      "244/244 [==============================] - 140s 572ms/step - loss: 1.3116 - accuracy: 0.5846\n",
      "Epoch 93/1000\n",
      "244/244 [==============================] - 141s 577ms/step - loss: 1.3119 - accuracy: 0.5844\n",
      "Epoch 94/1000\n",
      "244/244 [==============================] - 144s 591ms/step - loss: 1.3094 - accuracy: 0.5847\n",
      "Epoch 95/1000\n",
      "244/244 [==============================] - 145s 593ms/step - loss: 1.3094 - accuracy: 0.5857\n",
      "Epoch 96/1000\n",
      "244/244 [==============================] - 152s 623ms/step - loss: 1.3073 - accuracy: 0.5859\n",
      "Epoch 97/1000\n",
      "244/244 [==============================] - 142s 582ms/step - loss: 1.3064 - accuracy: 0.5863\n",
      "Epoch 98/1000\n",
      "244/244 [==============================] - 142s 581ms/step - loss: 1.3046 - accuracy: 0.5861\n",
      "Epoch 99/1000\n",
      "244/244 [==============================] - 142s 582ms/step - loss: 1.3044 - accuracy: 0.5864\n",
      "Epoch 100/1000\n",
      "244/244 [==============================] - 157s 642ms/step - loss: 1.3057 - accuracy: 0.5860\n",
      "Epoch 101/1000\n",
      "244/244 [==============================] - 153s 629ms/step - loss: 1.3036 - accuracy: 0.5869\n",
      "Epoch 102/1000\n",
      "244/244 [==============================] - 151s 617ms/step - loss: 1.3023 - accuracy: 0.5869\n",
      "Epoch 103/1000\n",
      "244/244 [==============================] - 144s 592ms/step - loss: 1.3032 - accuracy: 0.5864\n",
      "Epoch 104/1000\n",
      "244/244 [==============================] - 143s 588ms/step - loss: 1.3039 - accuracy: 0.5869\n",
      "Epoch 105/1000\n",
      "244/244 [==============================] - 173s 711ms/step - loss: 1.3027 - accuracy: 0.5867\n",
      "Epoch 106/1000\n",
      "244/244 [==============================] - 160s 658ms/step - loss: 1.2997 - accuracy: 0.5877\n",
      "Epoch 107/1000\n",
      "244/244 [==============================] - 147s 601ms/step - loss: 1.2985 - accuracy: 0.5882\n",
      "Epoch 108/1000\n",
      "244/244 [==============================] - 156s 640ms/step - loss: 1.3002 - accuracy: 0.5878\n",
      "Epoch 109/1000\n",
      "244/244 [==============================] - 158s 647ms/step - loss: 1.3014 - accuracy: 0.5877\n",
      "Epoch 110/1000\n",
      "244/244 [==============================] - 150s 615ms/step - loss: 1.2975 - accuracy: 0.5885\n",
      "Epoch 111/1000\n",
      "244/244 [==============================] - 163s 669ms/step - loss: 1.2975 - accuracy: 0.5885\n",
      "Epoch 112/1000\n",
      "244/244 [==============================] - 148s 606ms/step - loss: 1.2972 - accuracy: 0.5884\n",
      "Epoch 113/1000\n",
      "244/244 [==============================] - 143s 588ms/step - loss: 1.2976 - accuracy: 0.5888\n",
      "Epoch 114/1000\n",
      "244/244 [==============================] - 148s 608ms/step - loss: 1.2981 - accuracy: 0.5882\n",
      "Epoch 115/1000\n",
      "244/244 [==============================] - 142s 582ms/step - loss: 1.2951 - accuracy: 0.5894\n",
      "Epoch 116/1000\n",
      "244/244 [==============================] - 144s 590ms/step - loss: 1.2956 - accuracy: 0.5894\n",
      "Epoch 117/1000\n",
      "244/244 [==============================] - 142s 584ms/step - loss: 1.2946 - accuracy: 0.5895\n",
      "Epoch 118/1000\n",
      "244/244 [==============================] - 141s 579ms/step - loss: 1.2946 - accuracy: 0.5898\n",
      "Epoch 119/1000\n",
      "244/244 [==============================] - 139s 570ms/step - loss: 1.2918 - accuracy: 0.5906\n",
      "Epoch 120/1000\n",
      "244/244 [==============================] - 157s 643ms/step - loss: 1.2940 - accuracy: 0.5897\n",
      "Epoch 121/1000\n",
      "244/244 [==============================] - 144s 591ms/step - loss: 1.2930 - accuracy: 0.5899\n",
      "Epoch 122/1000\n",
      "244/244 [==============================] - 146s 597ms/step - loss: 1.2925 - accuracy: 0.5904\n",
      "Epoch 123/1000\n",
      "244/244 [==============================] - 145s 592ms/step - loss: 1.2912 - accuracy: 0.5906\n",
      "Epoch 124/1000\n",
      "244/244 [==============================] - 144s 589ms/step - loss: 1.2918 - accuracy: 0.5905\n",
      "Epoch 125/1000\n",
      "244/244 [==============================] - 151s 619ms/step - loss: 1.2915 - accuracy: 0.5911\n",
      "Epoch 126/1000\n",
      "244/244 [==============================] - 169s 694ms/step - loss: 1.2910 - accuracy: 0.5907\n",
      "Epoch 127/1000\n",
      "244/244 [==============================] - 150s 615ms/step - loss: 1.2871 - accuracy: 0.5920\n",
      "Epoch 128/1000\n",
      "244/244 [==============================] - 145s 594ms/step - loss: 1.2865 - accuracy: 0.5919\n",
      "Epoch 129/1000\n",
      "244/244 [==============================] - 145s 594ms/step - loss: 1.2902 - accuracy: 0.5912\n",
      "Epoch 130/1000\n",
      "244/244 [==============================] - 142s 583ms/step - loss: 1.2873 - accuracy: 0.5915\n",
      "Epoch 131/1000\n",
      "244/244 [==============================] - 141s 578ms/step - loss: 1.2876 - accuracy: 0.5920\n",
      "Epoch 132/1000\n",
      "244/244 [==============================] - 142s 583ms/step - loss: 1.2879 - accuracy: 0.5913\n",
      "Epoch 133/1000\n",
      "244/244 [==============================] - 140s 574ms/step - loss: 1.2829 - accuracy: 0.5936\n",
      "Epoch 134/1000\n",
      "244/244 [==============================] - 165s 677ms/step - loss: 1.2842 - accuracy: 0.5935\n",
      "Epoch 135/1000\n",
      "244/244 [==============================] - 151s 620ms/step - loss: 1.2849 - accuracy: 0.5924\n",
      "Epoch 136/1000\n",
      "244/244 [==============================] - 161s 660ms/step - loss: 1.2838 - accuracy: 0.5932\n",
      "Epoch 137/1000\n",
      "244/244 [==============================] - 165s 675ms/step - loss: 1.2828 - accuracy: 0.5936\n",
      "Epoch 138/1000\n",
      "244/244 [==============================] - 153s 628ms/step - loss: 1.2849 - accuracy: 0.5927\n",
      "Epoch 139/1000\n",
      "244/244 [==============================] - 153s 625ms/step - loss: 1.2827 - accuracy: 0.5934\n",
      "Epoch 140/1000\n",
      "244/244 [==============================] - 154s 632ms/step - loss: 1.2829 - accuracy: 0.5934\n",
      "Epoch 141/1000\n",
      "244/244 [==============================] - 151s 618ms/step - loss: 1.2854 - accuracy: 0.5920\n",
      "Epoch 142/1000\n",
      "244/244 [==============================] - 16089s 66s/step - loss: 1.2814 - accuracy: 0.5940\n",
      "Epoch 143/1000\n",
      "244/244 [==============================] - 138s 567ms/step - loss: 1.2832 - accuracy: 0.5932\n",
      "Epoch 144/1000\n",
      "244/244 [==============================] - 136s 558ms/step - loss: 1.2825 - accuracy: 0.5938\n",
      "Epoch 145/1000\n",
      "244/244 [==============================] - 143s 588ms/step - loss: 1.2817 - accuracy: 0.5940\n",
      "Epoch 146/1000\n",
      "244/244 [==============================] - 136s 557ms/step - loss: 1.2810 - accuracy: 0.5943\n",
      "Epoch 147/1000\n",
      "244/244 [==============================] - 138s 565ms/step - loss: 1.2800 - accuracy: 0.5941\n",
      "Epoch 148/1000\n",
      "244/244 [==============================] - 140s 576ms/step - loss: 1.2808 - accuracy: 0.5939\n",
      "Epoch 149/1000\n",
      "244/244 [==============================] - 141s 577ms/step - loss: 1.2784 - accuracy: 0.5949\n",
      "Epoch 150/1000\n",
      "244/244 [==============================] - 145s 596ms/step - loss: 1.2822 - accuracy: 0.5939\n",
      "Epoch 151/1000\n",
      "244/244 [==============================] - 144s 591ms/step - loss: 1.2782 - accuracy: 0.5946\n",
      "Epoch 152/1000\n",
      "244/244 [==============================] - 145s 596ms/step - loss: 1.2799 - accuracy: 0.5945\n",
      "Epoch 153/1000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "244/244 [==============================] - 145s 595ms/step - loss: 1.2779 - accuracy: 0.5950\n",
      "Epoch 154/1000\n",
      "244/244 [==============================] - 143s 586ms/step - loss: 1.2774 - accuracy: 0.5948\n",
      "Epoch 155/1000\n",
      "244/244 [==============================] - 147s 604ms/step - loss: 1.2768 - accuracy: 0.5953\n",
      "Epoch 156/1000\n",
      "244/244 [==============================] - 148s 606ms/step - loss: 1.2767 - accuracy: 0.5962\n",
      "Epoch 157/1000\n",
      "244/244 [==============================] - 144s 592ms/step - loss: 1.2771 - accuracy: 0.5954\n",
      "Epoch 158/1000\n",
      "244/244 [==============================] - 145s 595ms/step - loss: 1.2721 - accuracy: 0.5965\n",
      "Epoch 159/1000\n",
      "244/244 [==============================] - 146s 597ms/step - loss: 1.2755 - accuracy: 0.5959\n",
      "Epoch 160/1000\n",
      "244/244 [==============================] - 149s 609ms/step - loss: 1.2744 - accuracy: 0.5954\n",
      "Epoch 161/1000\n",
      "244/244 [==============================] - 153s 625ms/step - loss: 1.2737 - accuracy: 0.5961\n",
      "Epoch 162/1000\n",
      "244/244 [==============================] - 144s 591ms/step - loss: 1.2722 - accuracy: 0.5964\n",
      "Epoch 163/1000\n",
      "244/244 [==============================] - 147s 604ms/step - loss: 1.2740 - accuracy: 0.5962\n",
      "Epoch 164/1000\n",
      "244/244 [==============================] - 159s 652ms/step - loss: 1.2714 - accuracy: 0.5960\n",
      "Epoch 165/1000\n",
      "244/244 [==============================] - 174s 714ms/step - loss: 1.2731 - accuracy: 0.5966\n",
      "Epoch 166/1000\n",
      "244/244 [==============================] - 170s 696ms/step - loss: 1.2737 - accuracy: 0.5968\n",
      "Epoch 167/1000\n",
      "244/244 [==============================] - 166s 681ms/step - loss: 1.2702 - accuracy: 0.5974\n",
      "Epoch 168/1000\n",
      "244/244 [==============================] - 159s 652ms/step - loss: 1.2746 - accuracy: 0.5954\n",
      "Epoch 169/1000\n",
      "244/244 [==============================] - 144s 592ms/step - loss: 1.2676 - accuracy: 0.5981\n",
      "Epoch 170/1000\n",
      "244/244 [==============================] - 150s 615ms/step - loss: 1.2732 - accuracy: 0.5961\n",
      "Epoch 171/1000\n",
      "244/244 [==============================] - 145s 593ms/step - loss: 1.2715 - accuracy: 0.5974\n",
      "Epoch 172/1000\n",
      "244/244 [==============================] - 146s 598ms/step - loss: 1.2700 - accuracy: 0.5966\n",
      "Epoch 173/1000\n",
      "244/244 [==============================] - 147s 603ms/step - loss: 1.2704 - accuracy: 0.5971\n",
      "Epoch 174/1000\n",
      "244/244 [==============================] - 147s 601ms/step - loss: 1.2685 - accuracy: 0.5990\n",
      "Epoch 175/1000\n",
      "244/244 [==============================] - 144s 591ms/step - loss: 1.2682 - accuracy: 0.5974\n",
      "Epoch 176/1000\n",
      "244/244 [==============================] - 142s 584ms/step - loss: 1.2670 - accuracy: 0.5975\n",
      "Epoch 177/1000\n",
      "244/244 [==============================] - 143s 585ms/step - loss: 1.2674 - accuracy: 0.5980\n",
      "Epoch 178/1000\n",
      "244/244 [==============================] - 144s 589ms/step - loss: 1.2664 - accuracy: 0.5983\n",
      "Epoch 179/1000\n",
      "244/244 [==============================] - 143s 586ms/step - loss: 1.2663 - accuracy: 0.5980\n",
      "Epoch 180/1000\n",
      "244/244 [==============================] - 143s 588ms/step - loss: 1.2685 - accuracy: 0.5979\n",
      "Epoch 181/1000\n",
      "244/244 [==============================] - 144s 590ms/step - loss: 1.2675 - accuracy: 0.5983\n",
      "Epoch 182/1000\n",
      "244/244 [==============================] - 145s 593ms/step - loss: 1.2659 - accuracy: 0.5984\n",
      "Epoch 183/1000\n",
      "244/244 [==============================] - 147s 601ms/step - loss: 1.2671 - accuracy: 0.5983\n",
      "Epoch 184/1000\n",
      "244/244 [==============================] - 146s 600ms/step - loss: 1.2659 - accuracy: 0.5983\n",
      "Epoch 185/1000\n",
      "244/244 [==============================] - 147s 601ms/step - loss: 1.2661 - accuracy: 0.5984\n",
      "Epoch 186/1000\n",
      "244/244 [==============================] - 146s 599ms/step - loss: 1.2649 - accuracy: 0.5987\n",
      "Epoch 187/1000\n",
      "244/244 [==============================] - 148s 605ms/step - loss: 1.2652 - accuracy: 0.5998\n",
      "Epoch 188/1000\n",
      "244/244 [==============================] - 146s 598ms/step - loss: 1.2669 - accuracy: 0.5982\n",
      "Epoch 189/1000\n",
      "244/244 [==============================] - 145s 594ms/step - loss: 1.2627 - accuracy: 0.5996\n",
      "Epoch 190/1000\n",
      "244/244 [==============================] - 145s 593ms/step - loss: 1.2664 - accuracy: 0.5982\n",
      "Epoch 191/1000\n",
      "244/244 [==============================] - 145s 594ms/step - loss: 1.2611 - accuracy: 0.6001\n",
      "Epoch 192/1000\n",
      "244/244 [==============================] - 144s 591ms/step - loss: 1.2614 - accuracy: 0.6000\n",
      "Epoch 193/1000\n",
      "244/244 [==============================] - 145s 594ms/step - loss: 1.2617 - accuracy: 0.5999\n",
      "Epoch 194/1000\n",
      "244/244 [==============================] - 145s 595ms/step - loss: 1.2636 - accuracy: 0.5993\n",
      "Epoch 195/1000\n",
      "244/244 [==============================] - 147s 602ms/step - loss: 1.2608 - accuracy: 0.5998\n",
      "Epoch 196/1000\n",
      "244/244 [==============================] - 150s 615ms/step - loss: 1.2619 - accuracy: 0.5992\n",
      "Epoch 197/1000\n",
      "244/244 [==============================] - 2896s 12s/step - loss: 1.2592 - accuracy: 0.6003\n",
      "Epoch 198/1000\n",
      "244/244 [==============================] - 160s 652ms/step - loss: 1.2602 - accuracy: 0.6005\n",
      "Epoch 199/1000\n",
      "244/244 [==============================] - 147s 602ms/step - loss: 1.2656 - accuracy: 0.5992\n",
      "Epoch 200/1000\n",
      "244/244 [==============================] - 184s 755ms/step - loss: 1.2604 - accuracy: 0.6001\n",
      "Epoch 201/1000\n",
      "244/244 [==============================] - 141s 578ms/step - loss: 1.2627 - accuracy: 0.5997\n",
      "Epoch 202/1000\n",
      "244/244 [==============================] - 205s 841ms/step - loss: 1.2570 - accuracy: 0.6010\n",
      "Epoch 203/1000\n",
      "244/244 [==============================] - 148s 606ms/step - loss: 1.2598 - accuracy: 0.6005\n",
      "Epoch 204/1000\n",
      "244/244 [==============================] - 141s 578ms/step - loss: 1.2593 - accuracy: 0.6007\n",
      "Epoch 205/1000\n",
      "244/244 [==============================] - 157s 642ms/step - loss: 1.2568 - accuracy: 0.6009\n",
      "Epoch 206/1000\n",
      "244/244 [==============================] - 162s 665ms/step - loss: 1.2613 - accuracy: 0.5999\n",
      "Epoch 207/1000\n",
      "244/244 [==============================] - 149s 608ms/step - loss: 1.2578 - accuracy: 0.6002\n",
      "Epoch 208/1000\n",
      "244/244 [==============================] - 148s 606ms/step - loss: 1.2579 - accuracy: 0.6010\n",
      "Epoch 209/1000\n",
      "244/244 [==============================] - 150s 614ms/step - loss: 1.2573 - accuracy: 0.6014\n",
      "Epoch 210/1000\n",
      "244/244 [==============================] - 146s 600ms/step - loss: 1.2608 - accuracy: 0.6001\n",
      "Epoch 211/1000\n",
      "244/244 [==============================] - 143s 584ms/step - loss: 1.2608 - accuracy: 0.6005\n",
      "Epoch 212/1000\n",
      "244/244 [==============================] - 171s 701ms/step - loss: 1.2571 - accuracy: 0.6007\n",
      "Epoch 213/1000\n",
      "244/244 [==============================] - 203s 835ms/step - loss: 1.2579 - accuracy: 0.6009\n",
      "Epoch 214/1000\n",
      "244/244 [==============================] - 181s 741ms/step - loss: 1.2544 - accuracy: 0.6021\n",
      "Epoch 215/1000\n",
      "244/244 [==============================] - 139s 568ms/step - loss: 1.2562 - accuracy: 0.6015\n",
      "Epoch 216/1000\n",
      "244/244 [==============================] - 141s 577ms/step - loss: 1.2545 - accuracy: 0.6020\n",
      "Epoch 217/1000\n",
      "244/244 [==============================] - 140s 574ms/step - loss: 1.2550 - accuracy: 0.6022\n",
      "Epoch 218/1000\n",
      "244/244 [==============================] - 178s 730ms/step - loss: 1.2560 - accuracy: 0.6018\n",
      "Epoch 219/1000\n",
      "244/244 [==============================] - 140s 572ms/step - loss: 1.2546 - accuracy: 0.6017\n",
      "Epoch 220/1000\n",
      "244/244 [==============================] - 144s 589ms/step - loss: 1.2535 - accuracy: 0.6029\n",
      "Epoch 221/1000\n",
      "244/244 [==============================] - 142s 581ms/step - loss: 1.2582 - accuracy: 0.6006\n",
      "Epoch 222/1000\n",
      "244/244 [==============================] - 150s 614ms/step - loss: 1.2536 - accuracy: 0.6021\n",
      "Epoch 223/1000\n",
      "244/244 [==============================] - 180s 740ms/step - loss: 1.2519 - accuracy: 0.6032\n",
      "Epoch 224/1000\n",
      "244/244 [==============================] - 188s 772ms/step - loss: 1.2518 - accuracy: 0.6034\n",
      "Epoch 225/1000\n",
      "244/244 [==============================] - 198s 809ms/step - loss: 1.2542 - accuracy: 0.6026\n",
      "Epoch 226/1000\n",
      "244/244 [==============================] - 182s 748ms/step - loss: 1.2529 - accuracy: 0.6029\n",
      "Epoch 227/1000\n",
      "244/244 [==============================] - 158s 647ms/step - loss: 1.2510 - accuracy: 0.6033\n",
      "Epoch 228/1000\n",
      "244/244 [==============================] - 195s 799ms/step - loss: 1.2522 - accuracy: 0.6031\n",
      "Epoch 229/1000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "244/244 [==============================] - 190s 781ms/step - loss: 1.2523 - accuracy: 0.6034\n",
      "Epoch 230/1000\n",
      "244/244 [==============================] - 194s 797ms/step - loss: 1.2513 - accuracy: 0.6025\n",
      "Epoch 231/1000\n",
      "244/244 [==============================] - 180s 736ms/step - loss: 1.2521 - accuracy: 0.6027\n",
      "Epoch 232/1000\n",
      "244/244 [==============================] - 165s 675ms/step - loss: 1.2517 - accuracy: 0.6038\n",
      "Epoch 233/1000\n",
      "244/244 [==============================] - 154s 632ms/step - loss: 1.2523 - accuracy: 0.6033\n",
      "Epoch 234/1000\n",
      "244/244 [==============================] - 148s 606ms/step - loss: 1.2519 - accuracy: 0.6026\n",
      "Epoch 235/1000\n",
      "244/244 [==============================] - 165s 676ms/step - loss: 1.2506 - accuracy: 0.6033\n",
      "Epoch 236/1000\n",
      "244/244 [==============================] - 173s 709ms/step - loss: 1.2495 - accuracy: 0.6035\n",
      "Epoch 237/1000\n",
      "244/244 [==============================] - 172s 705ms/step - loss: 1.2473 - accuracy: 0.6044\n",
      "Epoch 238/1000\n",
      "244/244 [==============================] - 180s 737ms/step - loss: 1.2472 - accuracy: 0.6043\n",
      "Epoch 239/1000\n",
      "244/244 [==============================] - 179s 732ms/step - loss: 1.2502 - accuracy: 0.6039\n",
      "Epoch 240/1000\n",
      "244/244 [==============================] - 185s 757ms/step - loss: 1.2540 - accuracy: 0.6030\n",
      "Epoch 241/1000\n",
      "244/244 [==============================] - 174s 715ms/step - loss: 1.2495 - accuracy: 0.6036\n",
      "Epoch 242/1000\n",
      "244/244 [==============================] - 172s 707ms/step - loss: 1.2478 - accuracy: 0.6047\n",
      "Epoch 243/1000\n",
      "244/244 [==============================] - 178s 729ms/step - loss: 1.2507 - accuracy: 0.6037\n",
      "Epoch 244/1000\n",
      "244/244 [==============================] - 162s 663ms/step - loss: 1.2508 - accuracy: 0.6027\n",
      "Epoch 245/1000\n",
      "244/244 [==============================] - 171s 701ms/step - loss: 1.2472 - accuracy: 0.6044\n",
      "Epoch 246/1000\n",
      "244/244 [==============================] - 146s 598ms/step - loss: 1.2478 - accuracy: 0.6035\n",
      "Epoch 247/1000\n",
      "244/244 [==============================] - 158s 648ms/step - loss: 1.2544 - accuracy: 0.6023\n",
      "Epoch 248/1000\n",
      "244/244 [==============================] - 159s 654ms/step - loss: 1.2484 - accuracy: 0.6041\n",
      "Epoch 249/1000\n",
      "244/244 [==============================] - 143s 587ms/step - loss: 1.2474 - accuracy: 0.6046\n",
      "Epoch 250/1000\n",
      "244/244 [==============================] - 144s 592ms/step - loss: 1.2480 - accuracy: 0.6041\n",
      "Epoch 251/1000\n",
      "244/244 [==============================] - 151s 618ms/step - loss: 1.2473 - accuracy: 0.6041\n",
      "Epoch 252/1000\n",
      "244/244 [==============================] - 143s 584ms/step - loss: 1.2434 - accuracy: 0.6057\n",
      "Epoch 253/1000\n",
      "244/244 [==============================] - 146s 597ms/step - loss: 1.2456 - accuracy: 0.6050\n",
      "Epoch 254/1000\n",
      "244/244 [==============================] - 141s 580ms/step - loss: 1.2455 - accuracy: 0.6045\n",
      "Epoch 255/1000\n",
      "244/244 [==============================] - 143s 588ms/step - loss: 1.2473 - accuracy: 0.6044\n",
      "Epoch 256/1000\n",
      "244/244 [==============================] - 144s 590ms/step - loss: 1.2450 - accuracy: 0.6048\n",
      "Epoch 257/1000\n",
      "244/244 [==============================] - 145s 596ms/step - loss: 1.2443 - accuracy: 0.6048\n",
      "Epoch 258/1000\n",
      "244/244 [==============================] - 143s 584ms/step - loss: 1.2427 - accuracy: 0.6061\n",
      "Epoch 259/1000\n",
      "244/244 [==============================] - 144s 589ms/step - loss: 1.2451 - accuracy: 0.6050\n",
      "Epoch 260/1000\n",
      "244/244 [==============================] - 143s 586ms/step - loss: 1.2471 - accuracy: 0.6052\n",
      "Epoch 261/1000\n",
      "244/244 [==============================] - 143s 586ms/step - loss: 1.2453 - accuracy: 0.6050\n",
      "Epoch 262/1000\n",
      "244/244 [==============================] - 142s 582ms/step - loss: 1.2452 - accuracy: 0.6057\n",
      "Epoch 263/1000\n",
      "244/244 [==============================] - 145s 593ms/step - loss: 1.2422 - accuracy: 0.6056\n",
      "Epoch 264/1000\n",
      "244/244 [==============================] - 143s 585ms/step - loss: 1.2438 - accuracy: 0.6060\n",
      "Epoch 265/1000\n",
      "244/244 [==============================] - 143s 588ms/step - loss: 1.2421 - accuracy: 0.6062\n",
      "Epoch 266/1000\n",
      "244/244 [==============================] - 143s 588ms/step - loss: 1.2417 - accuracy: 0.6064\n",
      "Epoch 267/1000\n",
      "244/244 [==============================] - 141s 579ms/step - loss: 1.2408 - accuracy: 0.6068\n",
      "Epoch 268/1000\n",
      "244/244 [==============================] - 142s 583ms/step - loss: 1.2694 - accuracy: 0.5981\n",
      "Epoch 269/1000\n",
      "244/244 [==============================] - 63936s 263s/step - loss: 1.2507 - accuracy: 0.6032\n",
      "Epoch 270/1000\n",
      "244/244 [==============================] - 197s 805ms/step - loss: 1.2864 - accuracy: 0.5943\n",
      "Epoch 271/1000\n",
      "244/244 [==============================] - 183s 750ms/step - loss: 1.2457 - accuracy: 0.6044\n",
      "Epoch 272/1000\n",
      "244/244 [==============================] - 156s 639ms/step - loss: 1.2393 - accuracy: 0.6069\n",
      "Epoch 273/1000\n",
      "244/244 [==============================] - 150s 614ms/step - loss: 1.2382 - accuracy: 0.6069\n",
      "Epoch 274/1000\n",
      "244/244 [==============================] - 147s 604ms/step - loss: 1.2391 - accuracy: 0.6066\n",
      "Epoch 275/1000\n",
      "244/244 [==============================] - 164s 671ms/step - loss: 1.2394 - accuracy: 0.6073\n",
      "Epoch 276/1000\n",
      "244/244 [==============================] - 182s 744ms/step - loss: 1.2380 - accuracy: 0.6072\n",
      "Epoch 277/1000\n",
      "244/244 [==============================] - 179s 732ms/step - loss: 1.2406 - accuracy: 0.6060\n",
      "Epoch 278/1000\n",
      "244/244 [==============================] - 153s 625ms/step - loss: 1.2412 - accuracy: 0.6065\n",
      "Epoch 279/1000\n",
      "244/244 [==============================] - 155s 636ms/step - loss: 1.2397 - accuracy: 0.6069\n",
      "Epoch 280/1000\n",
      "244/244 [==============================] - 148s 606ms/step - loss: 1.2384 - accuracy: 0.6071\n",
      "Epoch 281/1000\n",
      "244/244 [==============================] - 150s 617ms/step - loss: 1.2418 - accuracy: 0.6064\n",
      "Epoch 282/1000\n",
      "244/244 [==============================] - 172s 705ms/step - loss: 1.2386 - accuracy: 0.6071\n",
      "Epoch 283/1000\n",
      "244/244 [==============================] - 147s 603ms/step - loss: 1.2390 - accuracy: 0.6071\n",
      "Epoch 284/1000\n",
      "244/244 [==============================] - 147s 604ms/step - loss: 1.2383 - accuracy: 0.6074\n",
      "Epoch 285/1000\n",
      "244/244 [==============================] - 151s 618ms/step - loss: 1.2650 - accuracy: 0.5997\n",
      "Epoch 286/1000\n",
      "244/244 [==============================] - 141s 577ms/step - loss: 1.2398 - accuracy: 0.6067\n",
      "Epoch 287/1000\n",
      "244/244 [==============================] - 142s 582ms/step - loss: 1.2394 - accuracy: 0.6070\n",
      "Epoch 288/1000\n",
      "244/244 [==============================] - 141s 580ms/step - loss: 1.2371 - accuracy: 0.6075\n",
      "Epoch 289/1000\n",
      "244/244 [==============================] - 142s 580ms/step - loss: 1.2381 - accuracy: 0.6070\n",
      "Epoch 290/1000\n",
      "244/244 [==============================] - 141s 576ms/step - loss: 1.2371 - accuracy: 0.6078\n",
      "Epoch 291/1000\n",
      "244/244 [==============================] - 142s 581ms/step - loss: 1.2392 - accuracy: 0.6072\n",
      "Epoch 292/1000\n",
      "244/244 [==============================] - 146s 599ms/step - loss: 1.2424 - accuracy: 0.6067\n",
      "Epoch 293/1000\n",
      "244/244 [==============================] - 143s 586ms/step - loss: 1.2393 - accuracy: 0.6071\n",
      "Epoch 294/1000\n",
      "244/244 [==============================] - 141s 578ms/step - loss: 1.2365 - accuracy: 0.6080\n",
      "Epoch 295/1000\n",
      "244/244 [==============================] - 143s 587ms/step - loss: 1.2358 - accuracy: 0.6079\n",
      "Epoch 296/1000\n",
      "244/244 [==============================] - 142s 580ms/step - loss: 1.2371 - accuracy: 0.6077\n",
      "Epoch 297/1000\n",
      "244/244 [==============================] - 146s 599ms/step - loss: 1.2337 - accuracy: 0.6090\n",
      "Epoch 298/1000\n",
      "244/244 [==============================] - 147s 602ms/step - loss: 1.2363 - accuracy: 0.6076\n",
      "Epoch 299/1000\n",
      "244/244 [==============================] - 166s 682ms/step - loss: 1.2376 - accuracy: 0.6066\n",
      "Epoch 300/1000\n",
      "244/244 [==============================] - 174s 712ms/step - loss: 1.2366 - accuracy: 0.6074\n",
      "Epoch 301/1000\n",
      "244/244 [==============================] - 173s 708ms/step - loss: 1.2372 - accuracy: 0.6084\n",
      "Epoch 302/1000\n",
      "244/244 [==============================] - 201s 826ms/step - loss: 1.2343 - accuracy: 0.6080\n",
      "Epoch 303/1000\n",
      "244/244 [==============================] - 205s 840ms/step - loss: 1.2360 - accuracy: 0.6081\n",
      "Epoch 304/1000\n",
      "244/244 [==============================] - 201s 824ms/step - loss: 1.2353 - accuracy: 0.6087\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 305/1000\n",
      "244/244 [==============================] - 197s 806ms/step - loss: 1.2347 - accuracy: 0.6091\n",
      "Epoch 306/1000\n",
      "244/244 [==============================] - 161s 660ms/step - loss: 1.2347 - accuracy: 0.6086\n",
      "Epoch 307/1000\n",
      "244/244 [==============================] - 186s 764ms/step - loss: 1.2349 - accuracy: 0.6083\n",
      "Epoch 308/1000\n",
      "244/244 [==============================] - 154s 632ms/step - loss: 1.2331 - accuracy: 0.6086\n",
      "Epoch 309/1000\n",
      "244/244 [==============================] - 154s 632ms/step - loss: 1.2329 - accuracy: 0.6088\n",
      "Epoch 310/1000\n",
      "244/244 [==============================] - 151s 620ms/step - loss: 1.2328 - accuracy: 0.6092\n",
      "Epoch 311/1000\n",
      "244/244 [==============================] - 173s 708ms/step - loss: 1.2354 - accuracy: 0.6080\n",
      "Epoch 312/1000\n",
      "244/244 [==============================] - 190s 777ms/step - loss: 1.2343 - accuracy: 0.6086\n",
      "Epoch 313/1000\n",
      "244/244 [==============================] - 198s 811ms/step - loss: 1.2310 - accuracy: 0.6094\n",
      "Epoch 314/1000\n",
      "244/244 [==============================] - 204s 836ms/step - loss: 1.2338 - accuracy: 0.6084\n",
      "Epoch 315/1000\n",
      "244/244 [==============================] - 191s 785ms/step - loss: 1.2306 - accuracy: 0.6092\n",
      "Epoch 316/1000\n",
      "244/244 [==============================] - 153s 628ms/step - loss: 1.2334 - accuracy: 0.6089\n",
      "Epoch 317/1000\n",
      "244/244 [==============================] - 186s 763ms/step - loss: 1.2330 - accuracy: 0.6097\n",
      "Epoch 318/1000\n",
      "244/244 [==============================] - 188s 770ms/step - loss: 1.2329 - accuracy: 0.6086\n",
      "Epoch 319/1000\n",
      "244/244 [==============================] - 182s 746ms/step - loss: 1.2300 - accuracy: 0.6094\n",
      "Epoch 320/1000\n",
      "244/244 [==============================] - 183s 750ms/step - loss: 1.2291 - accuracy: 0.6095\n",
      "Epoch 321/1000\n",
      "244/244 [==============================] - 181s 743ms/step - loss: 1.2311 - accuracy: 0.6090\n",
      "Epoch 322/1000\n",
      "244/244 [==============================] - 178s 729ms/step - loss: 1.2312 - accuracy: 0.6092\n",
      "Epoch 323/1000\n",
      "244/244 [==============================] - 180s 737ms/step - loss: 1.2336 - accuracy: 0.6087\n",
      "Epoch 324/1000\n",
      "244/244 [==============================] - 173s 709ms/step - loss: 1.2296 - accuracy: 0.6094\n",
      "Epoch 325/1000\n",
      "244/244 [==============================] - 172s 704ms/step - loss: 1.2314 - accuracy: 0.6093\n",
      "Epoch 326/1000\n",
      "244/244 [==============================] - 171s 701ms/step - loss: 1.2318 - accuracy: 0.6095\n",
      "Epoch 327/1000\n",
      "244/244 [==============================] - 185s 757ms/step - loss: 1.2310 - accuracy: 0.6098\n",
      "Epoch 328/1000\n",
      "244/244 [==============================] - 196s 804ms/step - loss: 1.2320 - accuracy: 0.6093\n",
      "Epoch 329/1000\n",
      "244/244 [==============================] - 201s 825ms/step - loss: 1.2300 - accuracy: 0.6104\n",
      "Epoch 330/1000\n",
      "244/244 [==============================] - 194s 796ms/step - loss: 1.2297 - accuracy: 0.6101\n",
      "Epoch 331/1000\n",
      "244/244 [==============================] - 192s 787ms/step - loss: 1.2291 - accuracy: 0.6099\n",
      "Epoch 332/1000\n",
      "244/244 [==============================] - 188s 771ms/step - loss: 1.2289 - accuracy: 0.6101\n",
      "Epoch 333/1000\n",
      "244/244 [==============================] - 195s 798ms/step - loss: 1.2292 - accuracy: 0.6094\n",
      "Epoch 334/1000\n",
      "244/244 [==============================] - 189s 773ms/step - loss: 1.2286 - accuracy: 0.6107\n",
      "Epoch 335/1000\n",
      "244/244 [==============================] - 185s 759ms/step - loss: 1.2288 - accuracy: 0.6102\n",
      "Epoch 336/1000\n",
      "244/244 [==============================] - 167s 685ms/step - loss: 1.2290 - accuracy: 0.6099\n",
      "Epoch 337/1000\n",
      "244/244 [==============================] - 142s 583ms/step - loss: 1.2297 - accuracy: 0.6101\n",
      "Epoch 338/1000\n",
      "244/244 [==============================] - 174s 716ms/step - loss: 1.2290 - accuracy: 0.6098\n",
      "Epoch 339/1000\n",
      "244/244 [==============================] - 167s 685ms/step - loss: 1.2295 - accuracy: 0.6094\n",
      "Epoch 340/1000\n",
      "244/244 [==============================] - 147s 603ms/step - loss: 1.2288 - accuracy: 0.6104\n",
      "Epoch 341/1000\n",
      "244/244 [==============================] - 144s 590ms/step - loss: 1.2265 - accuracy: 0.6110\n",
      "Epoch 342/1000\n",
      "244/244 [==============================] - 181s 741ms/step - loss: 1.2254 - accuracy: 0.6108\n",
      "Epoch 343/1000\n",
      "244/244 [==============================] - 184s 754ms/step - loss: 1.2276 - accuracy: 0.6111\n",
      "Epoch 344/1000\n",
      "244/244 [==============================] - 148s 605ms/step - loss: 1.2270 - accuracy: 0.6108\n",
      "Epoch 345/1000\n",
      "244/244 [==============================] - 190s 779ms/step - loss: 1.2280 - accuracy: 0.6104\n",
      "Epoch 346/1000\n",
      "244/244 [==============================] - 196s 805ms/step - loss: 1.2280 - accuracy: 0.6106\n",
      "Epoch 347/1000\n",
      "244/244 [==============================] - 195s 798ms/step - loss: 1.2249 - accuracy: 0.6117\n",
      "Epoch 348/1000\n",
      "244/244 [==============================] - 187s 767ms/step - loss: 1.2267 - accuracy: 0.6106\n",
      "Epoch 349/1000\n",
      "244/244 [==============================] - 175s 717ms/step - loss: 1.2252 - accuracy: 0.6111\n",
      "Epoch 350/1000\n",
      "244/244 [==============================] - 181s 742ms/step - loss: 1.2293 - accuracy: 0.6096\n",
      "Epoch 351/1000\n",
      "244/244 [==============================] - 165s 676ms/step - loss: 1.2241 - accuracy: 0.6120\n",
      "Epoch 352/1000\n",
      "244/244 [==============================] - 184s 755ms/step - loss: 1.2257 - accuracy: 0.6112\n",
      "Epoch 353/1000\n",
      "244/244 [==============================] - 185s 757ms/step - loss: 1.2221 - accuracy: 0.6118\n",
      "Epoch 354/1000\n",
      "244/244 [==============================] - 191s 782ms/step - loss: 1.2255 - accuracy: 0.6111\n",
      "Epoch 355/1000\n",
      "244/244 [==============================] - 178s 731ms/step - loss: 1.2266 - accuracy: 0.6110\n",
      "Epoch 356/1000\n",
      "244/244 [==============================] - 183s 749ms/step - loss: 1.2311 - accuracy: 0.6095\n",
      "Epoch 357/1000\n",
      "244/244 [==============================] - 172s 707ms/step - loss: 1.2269 - accuracy: 0.6110\n",
      "Epoch 358/1000\n",
      "244/244 [==============================] - 150s 613ms/step - loss: 1.2262 - accuracy: 0.6114\n",
      "Epoch 359/1000\n",
      "244/244 [==============================] - 192s 788ms/step - loss: 1.2252 - accuracy: 0.6117\n",
      "Epoch 360/1000\n",
      "244/244 [==============================] - 179s 732ms/step - loss: 1.2290 - accuracy: 0.6102\n",
      "Epoch 361/1000\n",
      "244/244 [==============================] - 193s 791ms/step - loss: 1.2271 - accuracy: 0.6107\n",
      "Epoch 362/1000\n",
      "244/244 [==============================] - 192s 786ms/step - loss: 1.2220 - accuracy: 0.6122\n",
      "Epoch 363/1000\n",
      "244/244 [==============================] - 192s 786ms/step - loss: 1.2251 - accuracy: 0.6113\n",
      "Epoch 364/1000\n",
      "244/244 [==============================] - 190s 780ms/step - loss: 1.2256 - accuracy: 0.6106\n",
      "Epoch 365/1000\n",
      "244/244 [==============================] - 185s 758ms/step - loss: 1.2254 - accuracy: 0.6117\n",
      "Epoch 366/1000\n",
      "244/244 [==============================] - 182s 747ms/step - loss: 1.2254 - accuracy: 0.6113\n",
      "Epoch 367/1000\n",
      "244/244 [==============================] - 182s 745ms/step - loss: 1.2229 - accuracy: 0.6123\n",
      "Epoch 368/1000\n",
      "244/244 [==============================] - 165s 676ms/step - loss: 1.2259 - accuracy: 0.6114\n",
      "Epoch 369/1000\n",
      "244/244 [==============================] - 143s 587ms/step - loss: 1.2237 - accuracy: 0.6119\n",
      "Epoch 370/1000\n",
      "244/244 [==============================] - 145s 595ms/step - loss: 1.2249 - accuracy: 0.6110\n",
      "Epoch 371/1000\n",
      "134/244 [===============>..............] - ETA: 14:54:38 - loss: 1.2269 - accuracy: 0.6114"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "\n",
    "\n",
    "Tx = 40\n",
    "Ty = 40\n",
    "n_a = 128  \n",
    "n_layers = 2  \n",
    "learning_rate = 0.01\n",
    "batch_size = 128\n",
    "num_epochs = 1000\n",
    "\n",
    "\n",
    "model = tf.keras.models.Sequential([\n",
    "    tf.keras.layers.LSTM(n_a, return_sequences=True, input_shape=(Tx, len(chars))),\n",
    "    tf.keras.layers.Dropout(0.5),\n",
    "    tf.keras.layers.LSTM(n_a, return_sequences=True),\n",
    "    tf.keras.layers.Dropout(0.5),\n",
    "    tf.keras.layers.Dense(len(chars), activation='softmax')\n",
    "])\n",
    "\n",
    "\n",
    "model.compile(optimizer=tf.keras.optimizers.Adam(learning_rate=learning_rate),\n",
    "              loss='categorical_crossentropy', metrics=['accuracy'])\n",
    "\n",
    "\n",
    "history = model.fit(X, Y, batch_size=batch_size, epochs=num_epochs)\n",
    "model.save('shakespeare_model.h5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow.keras.layers import Input, LSTM, Dense, Dropout, Embedding\n",
    "from tensorflow.keras.models import Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "Input 0 of layer lstm is incompatible with the layer: expected ndim=3, found ndim=4. Full shape received: (None, 40, 38, 128)",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-8-6470c8dd4b77>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      7\u001b[0m \u001b[1;32mif\u001b[0m \u001b[0membedding_size\u001b[0m \u001b[1;33m>\u001b[0m \u001b[1;36m0\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      8\u001b[0m     \u001b[0membedding_layer\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mEmbedding\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mvocab_size\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0membedding_size\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0minput_layer\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 9\u001b[1;33m     \u001b[0mlstm_layer\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mLSTM\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mhidden_size\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mreturn_sequences\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mTrue\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0membedding_layer\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     10\u001b[0m \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     11\u001b[0m     \u001b[0mlstm_layer\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mLSTM\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mhidden_size\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mreturn_sequences\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mTrue\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0minput_layer\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Roaming\\Python\\Python37\\site-packages\\tensorflow\\python\\keras\\layers\\recurrent.py\u001b[0m in \u001b[0;36m__call__\u001b[1;34m(self, inputs, initial_state, constants, **kwargs)\u001b[0m\n\u001b[0;32m    658\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    659\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[0minitial_state\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mNone\u001b[0m \u001b[1;32mand\u001b[0m \u001b[0mconstants\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 660\u001b[1;33m       \u001b[1;32mreturn\u001b[0m \u001b[0msuper\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mRNN\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m__call__\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0minputs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    661\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    662\u001b[0m     \u001b[1;31m# If any of `initial_state` or `constants` are specified and are Keras\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Roaming\\Python\\Python37\\site-packages\\tensorflow\\python\\keras\\engine\\base_layer.py\u001b[0m in \u001b[0;36m__call__\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m    950\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[0m_in_functional_construction_mode\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0minputs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mkwargs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0minput_list\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    951\u001b[0m       return self._functional_construction_call(inputs, args, kwargs,\n\u001b[1;32m--> 952\u001b[1;33m                                                 input_list)\n\u001b[0m\u001b[0;32m    953\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    954\u001b[0m     \u001b[1;31m# Maintains info about the `Layer.call` stack.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Roaming\\Python\\Python37\\site-packages\\tensorflow\\python\\keras\\engine\\base_layer.py\u001b[0m in \u001b[0;36m_functional_construction_call\u001b[1;34m(self, inputs, args, kwargs, input_list)\u001b[0m\n\u001b[0;32m   1089\u001b[0m         \u001b[1;31m# Check input assumptions set after layer building, e.g. input shape.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1090\u001b[0m         outputs = self._keras_tensor_symbolic_call(\n\u001b[1;32m-> 1091\u001b[1;33m             inputs, input_masks, args, kwargs)\n\u001b[0m\u001b[0;32m   1092\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1093\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0moutputs\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Roaming\\Python\\Python37\\site-packages\\tensorflow\\python\\keras\\engine\\base_layer.py\u001b[0m in \u001b[0;36m_keras_tensor_symbolic_call\u001b[1;34m(self, inputs, input_masks, args, kwargs)\u001b[0m\n\u001b[0;32m    820\u001b[0m       \u001b[1;32mreturn\u001b[0m \u001b[0mnest\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmap_structure\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mkeras_tensor\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mKerasTensor\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0moutput_signature\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    821\u001b[0m     \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 822\u001b[1;33m       \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_infer_output_signature\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0minputs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mkwargs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0minput_masks\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    823\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    824\u001b[0m   \u001b[1;32mdef\u001b[0m \u001b[0m_infer_output_signature\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0minputs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mkwargs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0minput_masks\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Roaming\\Python\\Python37\\site-packages\\tensorflow\\python\\keras\\engine\\base_layer.py\u001b[0m in \u001b[0;36m_infer_output_signature\u001b[1;34m(self, inputs, args, kwargs, input_masks)\u001b[0m\n\u001b[0;32m    860\u001b[0m           \u001b[1;31m# overridden).\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    861\u001b[0m           \u001b[1;31m# TODO(kaftan): do we maybe_build here, or have we already done it?\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 862\u001b[1;33m           \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_maybe_build\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0minputs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    863\u001b[0m           \u001b[0moutputs\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mcall_fn\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0minputs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    864\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Roaming\\Python\\Python37\\site-packages\\tensorflow\\python\\keras\\engine\\base_layer.py\u001b[0m in \u001b[0;36m_maybe_build\u001b[1;34m(self, inputs)\u001b[0m\n\u001b[0;32m   2683\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mbuilt\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   2684\u001b[0m       input_spec.assert_input_compatibility(\n\u001b[1;32m-> 2685\u001b[1;33m           self.input_spec, inputs, self.name)\n\u001b[0m\u001b[0;32m   2686\u001b[0m       \u001b[0minput_list\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mnest\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mflatten\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0minputs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   2687\u001b[0m       \u001b[1;32mif\u001b[0m \u001b[0minput_list\u001b[0m \u001b[1;32mand\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_dtype_policy\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcompute_dtype\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Roaming\\Python\\Python37\\site-packages\\tensorflow\\python\\keras\\engine\\input_spec.py\u001b[0m in \u001b[0;36massert_input_compatibility\u001b[1;34m(input_spec, inputs, layer_name)\u001b[0m\n\u001b[0;32m    221\u001b[0m                          \u001b[1;34m'expected ndim='\u001b[0m \u001b[1;33m+\u001b[0m \u001b[0mstr\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mspec\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mndim\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m+\u001b[0m \u001b[1;34m', found ndim='\u001b[0m \u001b[1;33m+\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    222\u001b[0m                          \u001b[0mstr\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mndim\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m+\u001b[0m \u001b[1;34m'. Full shape received: '\u001b[0m \u001b[1;33m+\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 223\u001b[1;33m                          str(tuple(shape)))\n\u001b[0m\u001b[0;32m    224\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[0mspec\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmax_ndim\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    225\u001b[0m       \u001b[0mndim\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mx\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mrank\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mValueError\u001b[0m: Input 0 of layer lstm is incompatible with the layer: expected ndim=3, found ndim=4. Full shape received: (None, 40, 38, 128)"
     ]
    }
   ],
   "source": [
    "Tx = 40\n",
    "vocab_size = 38\n",
    "embedding_size = 128\n",
    "hidden_size = 256\n",
    "\n",
    "input_layer = Input(shape=(Tx, vocab_size))\n",
    "if embedding_size > 0:\n",
    "    embedding_layer = Embedding(vocab_size, embedding_size)(input_layer)\n",
    "    lstm_layer = LSTM(hidden_size, return_sequences=True)(embedding_layer)\n",
    "else:\n",
    "    lstm_layer = LSTM(hidden_size, return_sequences=True)(input_layer)\n",
    "dropout_layer = Dropout(0.2)(lstm_layer)\n",
    "output_layer = Dense(vocab_size, activation='softmax')(dropout_layer)\n",
    "\n",
    "model = Model(inputs=input_layer, outputs=output_layer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.compile(optimizer='adam', loss='categorical_crossentropy')\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "Input 0 of layer lstm_4 is incompatible with the layer: expected ndim=3, found ndim=4. Full shape received: (None, 40, 38, 64)",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-18-b4e13ee81066>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     42\u001b[0m     \u001b[0mtf\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mkeras\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mlayers\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mLSTM\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mhidden_size\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mreturn_sequences\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mTrue\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     43\u001b[0m     \u001b[0mtf\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mkeras\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mlayers\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mDropout\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdropout_rate\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 44\u001b[1;33m     \u001b[0mtf\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mkeras\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mlayers\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mTimeDistributed\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtf\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mkeras\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mlayers\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mDense\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mvocab_size\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mactivation\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;34m'softmax'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     45\u001b[0m ])\n\u001b[0;32m     46\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Roaming\\Python\\Python37\\site-packages\\tensorflow\\python\\training\\tracking\\base.py\u001b[0m in \u001b[0;36m_method_wrapper\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m    515\u001b[0m     \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_self_setattr_tracking\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;32mFalse\u001b[0m  \u001b[1;31m# pylint: disable=protected-access\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    516\u001b[0m     \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 517\u001b[1;33m       \u001b[0mresult\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mmethod\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    518\u001b[0m     \u001b[1;32mfinally\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    519\u001b[0m       \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_self_setattr_tracking\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mprevious_value\u001b[0m  \u001b[1;31m# pylint: disable=protected-access\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Roaming\\Python\\Python37\\site-packages\\tensorflow\\python\\keras\\engine\\sequential.py\u001b[0m in \u001b[0;36m__init__\u001b[1;34m(self, layers, name)\u001b[0m\n\u001b[0;32m    142\u001b[0m         \u001b[0mlayers\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m[\u001b[0m\u001b[0mlayers\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    143\u001b[0m       \u001b[1;32mfor\u001b[0m \u001b[0mlayer\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mlayers\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 144\u001b[1;33m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0madd\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mlayer\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    145\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    146\u001b[0m   \u001b[1;33m@\u001b[0m\u001b[0mproperty\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Roaming\\Python\\Python37\\site-packages\\tensorflow\\python\\training\\tracking\\base.py\u001b[0m in \u001b[0;36m_method_wrapper\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m    515\u001b[0m     \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_self_setattr_tracking\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;32mFalse\u001b[0m  \u001b[1;31m# pylint: disable=protected-access\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    516\u001b[0m     \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 517\u001b[1;33m       \u001b[0mresult\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mmethod\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    518\u001b[0m     \u001b[1;32mfinally\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    519\u001b[0m       \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_self_setattr_tracking\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mprevious_value\u001b[0m  \u001b[1;31m# pylint: disable=protected-access\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Roaming\\Python\\Python37\\site-packages\\tensorflow\\python\\keras\\engine\\sequential.py\u001b[0m in \u001b[0;36madd\u001b[1;34m(self, layer)\u001b[0m\n\u001b[0;32m    221\u001b[0m       \u001b[1;31m# If the model is being built continuously on top of an input layer:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    222\u001b[0m       \u001b[1;31m# refresh its output.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 223\u001b[1;33m       \u001b[0moutput_tensor\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mlayer\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0moutputs\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    224\u001b[0m       \u001b[1;32mif\u001b[0m \u001b[0mlen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mnest\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mflatten\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0moutput_tensor\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m!=\u001b[0m \u001b[1;36m1\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    225\u001b[0m         \u001b[1;32mraise\u001b[0m \u001b[0mValueError\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mSINGLE_LAYER_OUTPUT_ERROR_MSG\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Roaming\\Python\\Python37\\site-packages\\tensorflow\\python\\keras\\layers\\recurrent.py\u001b[0m in \u001b[0;36m__call__\u001b[1;34m(self, inputs, initial_state, constants, **kwargs)\u001b[0m\n\u001b[0;32m    658\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    659\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[0minitial_state\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mNone\u001b[0m \u001b[1;32mand\u001b[0m \u001b[0mconstants\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 660\u001b[1;33m       \u001b[1;32mreturn\u001b[0m \u001b[0msuper\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mRNN\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m__call__\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0minputs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    661\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    662\u001b[0m     \u001b[1;31m# If any of `initial_state` or `constants` are specified and are Keras\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Roaming\\Python\\Python37\\site-packages\\tensorflow\\python\\keras\\engine\\base_layer.py\u001b[0m in \u001b[0;36m__call__\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m    950\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[0m_in_functional_construction_mode\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0minputs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mkwargs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0minput_list\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    951\u001b[0m       return self._functional_construction_call(inputs, args, kwargs,\n\u001b[1;32m--> 952\u001b[1;33m                                                 input_list)\n\u001b[0m\u001b[0;32m    953\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    954\u001b[0m     \u001b[1;31m# Maintains info about the `Layer.call` stack.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Roaming\\Python\\Python37\\site-packages\\tensorflow\\python\\keras\\engine\\base_layer.py\u001b[0m in \u001b[0;36m_functional_construction_call\u001b[1;34m(self, inputs, args, kwargs, input_list)\u001b[0m\n\u001b[0;32m   1089\u001b[0m         \u001b[1;31m# Check input assumptions set after layer building, e.g. input shape.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1090\u001b[0m         outputs = self._keras_tensor_symbolic_call(\n\u001b[1;32m-> 1091\u001b[1;33m             inputs, input_masks, args, kwargs)\n\u001b[0m\u001b[0;32m   1092\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1093\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0moutputs\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Roaming\\Python\\Python37\\site-packages\\tensorflow\\python\\keras\\engine\\base_layer.py\u001b[0m in \u001b[0;36m_keras_tensor_symbolic_call\u001b[1;34m(self, inputs, input_masks, args, kwargs)\u001b[0m\n\u001b[0;32m    820\u001b[0m       \u001b[1;32mreturn\u001b[0m \u001b[0mnest\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmap_structure\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mkeras_tensor\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mKerasTensor\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0moutput_signature\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    821\u001b[0m     \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 822\u001b[1;33m       \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_infer_output_signature\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0minputs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mkwargs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0minput_masks\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    823\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    824\u001b[0m   \u001b[1;32mdef\u001b[0m \u001b[0m_infer_output_signature\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0minputs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mkwargs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0minput_masks\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Roaming\\Python\\Python37\\site-packages\\tensorflow\\python\\keras\\engine\\base_layer.py\u001b[0m in \u001b[0;36m_infer_output_signature\u001b[1;34m(self, inputs, args, kwargs, input_masks)\u001b[0m\n\u001b[0;32m    860\u001b[0m           \u001b[1;31m# overridden).\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    861\u001b[0m           \u001b[1;31m# TODO(kaftan): do we maybe_build here, or have we already done it?\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 862\u001b[1;33m           \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_maybe_build\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0minputs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    863\u001b[0m           \u001b[0moutputs\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mcall_fn\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0minputs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    864\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Roaming\\Python\\Python37\\site-packages\\tensorflow\\python\\keras\\engine\\base_layer.py\u001b[0m in \u001b[0;36m_maybe_build\u001b[1;34m(self, inputs)\u001b[0m\n\u001b[0;32m   2683\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mbuilt\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   2684\u001b[0m       input_spec.assert_input_compatibility(\n\u001b[1;32m-> 2685\u001b[1;33m           self.input_spec, inputs, self.name)\n\u001b[0m\u001b[0;32m   2686\u001b[0m       \u001b[0minput_list\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mnest\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mflatten\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0minputs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   2687\u001b[0m       \u001b[1;32mif\u001b[0m \u001b[0minput_list\u001b[0m \u001b[1;32mand\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_dtype_policy\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcompute_dtype\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Roaming\\Python\\Python37\\site-packages\\tensorflow\\python\\keras\\engine\\input_spec.py\u001b[0m in \u001b[0;36massert_input_compatibility\u001b[1;34m(input_spec, inputs, layer_name)\u001b[0m\n\u001b[0;32m    221\u001b[0m                          \u001b[1;34m'expected ndim='\u001b[0m \u001b[1;33m+\u001b[0m \u001b[0mstr\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mspec\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mndim\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m+\u001b[0m \u001b[1;34m', found ndim='\u001b[0m \u001b[1;33m+\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    222\u001b[0m                          \u001b[0mstr\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mndim\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m+\u001b[0m \u001b[1;34m'. Full shape received: '\u001b[0m \u001b[1;33m+\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 223\u001b[1;33m                          str(tuple(shape)))\n\u001b[0m\u001b[0;32m    224\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[0mspec\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmax_ndim\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    225\u001b[0m       \u001b[0mndim\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mx\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mrank\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mValueError\u001b[0m: Input 0 of layer lstm_4 is incompatible with the layer: expected ndim=3, found ndim=4. Full shape received: (None, 40, 38, 64)"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "\n",
    "# Load and preprocess the dataset\n",
    "with open('shakespeare.txt') as f:\n",
    "    text = f.read()\n",
    "text = text.lower()  # case folding\n",
    "\n",
    "# Create character-level vocabulary\n",
    "chars = sorted(list(set(text)))\n",
    "vocab_size = len(chars)\n",
    "\n",
    "# Map characters to indices and vice versa\n",
    "char_to_index = {char: index for index, char in enumerate(chars)}\n",
    "index_to_char = {index: char for index, char in enumerate(chars)}\n",
    "\n",
    "# Create input and target sequences with a fixed window size\n",
    "Tx = 40  # input sequence length\n",
    "stride = 3  # stride to move the window\n",
    "sequences = []  # input sequences\n",
    "targets = []  # target sequences\n",
    "for i in range(0, len(text) - Tx, stride):\n",
    "    sequences.append(text[i:i+Tx])\n",
    "    targets.append(text[i+1:i+Tx+1])\n",
    "n_samples = len(sequences)\n",
    "\n",
    "# Convert sequences and targets to one-hot vectors\n",
    "X = np.zeros((n_samples, Tx, vocab_size), dtype=np.float32)\n",
    "Y = np.zeros((n_samples, Tx, vocab_size), dtype=np.float32)\n",
    "for i, sequence in enumerate(sequences):\n",
    "    for t, char in enumerate(sequence):\n",
    "        X[i, t, char_to_index[char]] = 1.0\n",
    "        Y[i, t, char_to_index[targets[i][t]]] = 1.0\n",
    "\n",
    "# Define the model architecture\n",
    "embedding_size = 64  # size of the character embedding\n",
    "hidden_size = 128  # size of the LSTM hidden state\n",
    "dropout_rate = 0.2  # dropout rate for regularization\n",
    "model = tf.keras.Sequential([\n",
    "    tf.keras.layers.Input(shape=(Tx, vocab_size)),\n",
    "    tf.keras.layers.Embedding(input_dim=vocab_size, output_dim=embedding_size),\n",
    "    tf.keras.layers.LSTM(hidden_size, return_sequences=True),\n",
    "    tf.keras.layers.Dropout(dropout_rate),\n",
    "    tf.keras.layers.TimeDistributed(tf.keras.layers.Dense(vocab_size, activation='softmax'))\n",
    "])\n",
    "\n",
    "# Compile the model\n",
    "optimizer = tf.keras.optimizers.Adam()\n",
    "loss = tf.keras.losses.CategoricalCrossentropy()\n",
    "model.compile(optimizer=optimizer, loss=loss)\n",
    "\n",
    "# Train the model\n",
    "batch_size = 128\n",
    "epochs = 1000\n",
    "history = model.fit(X, Y, batch_size=batch_size, epochs=epochs)\n",
    "\n",
    "# Generate text with the trained model\n",
    "seed_text = 'shall i compare thee to a summer\\'s day?\\n'\n",
    "generated_text = seed_text\n",
    "for i in range(1000):\n",
    "    x = np.zeros((1, Tx, vocab_size), dtype=np.float32)\n",
    "    for t, char in enumerate(generated_text[-Tx:]):\n",
    "        x[0, t, char_to_index[char]] = 1.0\n",
    "    y = model.predict(x)[0][-1]\n",
    "    next_char_index = np.argmax(y)\n",
    "    next_char = index_to_char[next_char_index]\n",
    "    generated_text += next_char\n",
    "\n",
    "print(generated_text)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of train samples: 31213\n",
      "Size of vocabulary: 38\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "# read the file and convert all characters to lowercase\n",
    "with open(\"shakespeare.txt\", \"r\") as f:\n",
    "    text = f.read().lower()\n",
    "\n",
    "# create a set of unique characters in the text\n",
    "chars = sorted(list(set(text)))\n",
    "\n",
    "# create a dictionary that maps each character to a unique integer\n",
    "char_to_int = {c: i for i, c in enumerate(chars)}\n",
    "\n",
    "# create a dictionary that maps each unique integer back to its corresponding character\n",
    "int_to_char = {i: c for i, c in enumerate(chars)}\n",
    "\n",
    "# define the input and output sequence lengths\n",
    "Tx = 40\n",
    "Ty = 40\n",
    "\n",
    "# create input and output sequences using a sliding window approach\n",
    "X = []\n",
    "Y = []\n",
    "for i in range(0, len(text) - Tx, 3):\n",
    "    X.append([char_to_int[c] for c in text[i:i+Tx]])\n",
    "    Y.append([char_to_int[c] for c in text[i+1:i+1+Ty]])\n",
    "\n",
    "# convert X and Y to numpy arrays\n",
    "X = np.array(X)\n",
    "Y = np.array(Y)\n",
    "\n",
    "# convert each character to a one-hot vector\n",
    "X = np.eye(len(chars))[X]\n",
    "Y = np.eye(len(chars))[Y]\n",
    "\n",
    "# print the number of train samples and the size of the vocabulary\n",
    "print(\"Number of train samples:\", X.shape[0])\n",
    "print(\"Size of vocabulary:\", len(chars))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[[0., 0., 0., ..., 0., 0., 0.],\n",
       "        [0., 0., 0., ..., 0., 0., 0.],\n",
       "        [0., 0., 0., ..., 0., 0., 0.],\n",
       "        ...,\n",
       "        [0., 0., 0., ..., 0., 0., 0.],\n",
       "        [0., 0., 0., ..., 0., 0., 0.],\n",
       "        [0., 0., 0., ..., 0., 0., 0.]],\n",
       "\n",
       "       [[0., 1., 0., ..., 0., 0., 0.],\n",
       "        [0., 0., 0., ..., 0., 0., 0.],\n",
       "        [0., 0., 0., ..., 0., 0., 0.],\n",
       "        ...,\n",
       "        [0., 0., 0., ..., 0., 0., 0.],\n",
       "        [0., 1., 0., ..., 0., 0., 0.],\n",
       "        [0., 0., 0., ..., 0., 0., 0.]],\n",
       "\n",
       "       [[0., 0., 0., ..., 0., 0., 0.],\n",
       "        [0., 0., 0., ..., 0., 0., 0.],\n",
       "        [0., 0., 0., ..., 0., 0., 0.],\n",
       "        ...,\n",
       "        [0., 0., 0., ..., 0., 0., 0.],\n",
       "        [0., 0., 0., ..., 0., 0., 0.],\n",
       "        [0., 0., 0., ..., 0., 0., 0.]],\n",
       "\n",
       "       ...,\n",
       "\n",
       "       [[0., 0., 0., ..., 0., 0., 0.],\n",
       "        [0., 1., 0., ..., 0., 0., 0.],\n",
       "        [0., 0., 0., ..., 0., 0., 0.],\n",
       "        ...,\n",
       "        [0., 1., 0., ..., 0., 0., 0.],\n",
       "        [0., 1., 0., ..., 0., 0., 0.],\n",
       "        [0., 1., 0., ..., 0., 0., 0.]],\n",
       "\n",
       "       [[0., 0., 0., ..., 0., 1., 0.],\n",
       "        [0., 1., 0., ..., 0., 0., 0.],\n",
       "        [0., 0., 0., ..., 0., 0., 0.],\n",
       "        ...,\n",
       "        [0., 0., 0., ..., 0., 0., 0.],\n",
       "        [0., 0., 0., ..., 0., 0., 0.],\n",
       "        [0., 0., 0., ..., 0., 0., 0.]],\n",
       "\n",
       "       [[0., 0., 0., ..., 0., 0., 0.],\n",
       "        [0., 0., 0., ..., 0., 0., 0.],\n",
       "        [0., 0., 0., ..., 0., 0., 0.],\n",
       "        ...,\n",
       "        [0., 1., 0., ..., 0., 0., 0.],\n",
       "        [0., 0., 0., ..., 0., 0., 0.],\n",
       "        [0., 0., 0., ..., 0., 0., 0.]]])"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[[0., 0., 0., ..., 0., 0., 0.],\n",
       "        [0., 0., 0., ..., 0., 0., 0.],\n",
       "        [0., 1., 0., ..., 0., 0., 0.],\n",
       "        ...,\n",
       "        [0., 0., 0., ..., 0., 0., 0.],\n",
       "        [0., 0., 0., ..., 0., 0., 0.],\n",
       "        [0., 0., 0., ..., 0., 0., 0.]],\n",
       "\n",
       "       [[0., 0., 0., ..., 0., 0., 0.],\n",
       "        [0., 0., 0., ..., 0., 0., 0.],\n",
       "        [0., 0., 0., ..., 0., 0., 0.],\n",
       "        ...,\n",
       "        [0., 1., 0., ..., 0., 0., 0.],\n",
       "        [0., 0., 0., ..., 0., 0., 0.],\n",
       "        [0., 0., 0., ..., 0., 0., 0.]],\n",
       "\n",
       "       [[0., 0., 0., ..., 0., 0., 0.],\n",
       "        [0., 0., 0., ..., 0., 0., 0.],\n",
       "        [0., 0., 0., ..., 0., 0., 0.],\n",
       "        ...,\n",
       "        [0., 0., 0., ..., 0., 0., 0.],\n",
       "        [0., 0., 0., ..., 0., 0., 0.],\n",
       "        [0., 0., 0., ..., 0., 0., 0.]],\n",
       "\n",
       "       ...,\n",
       "\n",
       "       [[0., 1., 0., ..., 0., 0., 0.],\n",
       "        [0., 0., 0., ..., 0., 0., 0.],\n",
       "        [0., 0., 0., ..., 0., 1., 0.],\n",
       "        ...,\n",
       "        [0., 1., 0., ..., 0., 0., 0.],\n",
       "        [0., 1., 0., ..., 0., 0., 0.],\n",
       "        [0., 0., 0., ..., 0., 0., 0.]],\n",
       "\n",
       "       [[0., 1., 0., ..., 0., 0., 0.],\n",
       "        [0., 0., 0., ..., 0., 0., 0.],\n",
       "        [0., 0., 0., ..., 0., 0., 0.],\n",
       "        ...,\n",
       "        [0., 0., 0., ..., 0., 0., 0.],\n",
       "        [0., 0., 0., ..., 0., 0., 0.],\n",
       "        [0., 1., 0., ..., 0., 0., 0.]],\n",
       "\n",
       "       [[0., 0., 0., ..., 0., 0., 0.],\n",
       "        [0., 0., 0., ..., 0., 0., 0.],\n",
       "        [0., 0., 0., ..., 0., 0., 0.],\n",
       "        ...,\n",
       "        [0., 0., 0., ..., 0., 0., 0.],\n",
       "        [0., 0., 0., ..., 0., 0., 0.],\n",
       "        [0., 0., 0., ..., 0., 0., 0.]]])"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Tx = 40\n",
    "vocab_size = 38\n",
    "embedding_size = 128\n",
    "hidden_size = 256\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/1000\n",
      "WARNING:tensorflow:Model was constructed with shape (None, None) for input KerasTensor(type_spec=TensorSpec(shape=(None, None), dtype=tf.float32, name='embedding_6_input'), name='embedding_6_input', description=\"created by layer 'embedding_6_input'\"), but it was called on an input with incompatible shape (None, 40, 38).\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "in user code:\n\n    C:\\Users\\Pinky\\AppData\\Roaming\\Python\\Python37\\site-packages\\tensorflow\\python\\keras\\engine\\training.py:805 train_function  *\n        return step_function(self, iterator)\n    C:\\Users\\Pinky\\AppData\\Roaming\\Python\\Python37\\site-packages\\tensorflow\\python\\keras\\engine\\training.py:795 step_function  **\n        outputs = model.distribute_strategy.run(run_step, args=(data,))\n    C:\\Users\\Pinky\\AppData\\Roaming\\Python\\Python37\\site-packages\\tensorflow\\python\\distribute\\distribute_lib.py:1259 run\n        return self._extended.call_for_each_replica(fn, args=args, kwargs=kwargs)\n    C:\\Users\\Pinky\\AppData\\Roaming\\Python\\Python37\\site-packages\\tensorflow\\python\\distribute\\distribute_lib.py:2730 call_for_each_replica\n        return self._call_for_each_replica(fn, args, kwargs)\n    C:\\Users\\Pinky\\AppData\\Roaming\\Python\\Python37\\site-packages\\tensorflow\\python\\distribute\\distribute_lib.py:3417 _call_for_each_replica\n        return fn(*args, **kwargs)\n    C:\\Users\\Pinky\\AppData\\Roaming\\Python\\Python37\\site-packages\\tensorflow\\python\\keras\\engine\\training.py:788 run_step  **\n        outputs = model.train_step(data)\n    C:\\Users\\Pinky\\AppData\\Roaming\\Python\\Python37\\site-packages\\tensorflow\\python\\keras\\engine\\training.py:754 train_step\n        y_pred = self(x, training=True)\n    C:\\Users\\Pinky\\AppData\\Roaming\\Python\\Python37\\site-packages\\tensorflow\\python\\keras\\engine\\base_layer.py:1012 __call__\n        outputs = call_fn(inputs, *args, **kwargs)\n    C:\\Users\\Pinky\\AppData\\Roaming\\Python\\Python37\\site-packages\\tensorflow\\python\\keras\\engine\\sequential.py:375 call\n        return super(Sequential, self).call(inputs, training=training, mask=mask)\n    C:\\Users\\Pinky\\AppData\\Roaming\\Python\\Python37\\site-packages\\tensorflow\\python\\keras\\engine\\functional.py:425 call\n        inputs, training=training, mask=mask)\n    C:\\Users\\Pinky\\AppData\\Roaming\\Python\\Python37\\site-packages\\tensorflow\\python\\keras\\engine\\functional.py:560 _run_internal_graph\n        outputs = node.layer(*args, **kwargs)\n    C:\\Users\\Pinky\\AppData\\Roaming\\Python\\Python37\\site-packages\\tensorflow\\python\\keras\\layers\\recurrent.py:660 __call__\n        return super(RNN, self).__call__(inputs, **kwargs)\n    C:\\Users\\Pinky\\AppData\\Roaming\\Python\\Python37\\site-packages\\tensorflow\\python\\keras\\engine\\base_layer.py:998 __call__\n        input_spec.assert_input_compatibility(self.input_spec, inputs, self.name)\n    C:\\Users\\Pinky\\AppData\\Roaming\\Python\\Python37\\site-packages\\tensorflow\\python\\keras\\engine\\input_spec.py:223 assert_input_compatibility\n        str(tuple(shape)))\n\n    ValueError: Input 0 of layer lstm_6 is incompatible with the layer: expected ndim=3, found ndim=4. Full shape received: (None, 40, 38, 128)\n",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-23-9029d49277eb>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     24\u001b[0m \u001b[0mmodel\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcompile\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0moptimizer\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0moptimizer\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mloss\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mloss_fn\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     25\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 26\u001b[1;33m \u001b[0mhistory\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mX\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mY\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mbatch_size\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mbatch_size\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mepochs\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mnum_epochs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;32m~\\AppData\\Roaming\\Python\\Python37\\site-packages\\tensorflow\\python\\keras\\engine\\training.py\u001b[0m in \u001b[0;36mfit\u001b[1;34m(self, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, steps_per_epoch, validation_steps, validation_batch_size, validation_freq, max_queue_size, workers, use_multiprocessing)\u001b[0m\n\u001b[0;32m   1098\u001b[0m                 _r=1):\n\u001b[0;32m   1099\u001b[0m               \u001b[0mcallbacks\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mon_train_batch_begin\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mstep\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1100\u001b[1;33m               \u001b[0mtmp_logs\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtrain_function\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0miterator\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1101\u001b[0m               \u001b[1;32mif\u001b[0m \u001b[0mdata_handler\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mshould_sync\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1102\u001b[0m                 \u001b[0mcontext\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0masync_wait\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Roaming\\Python\\Python37\\site-packages\\tensorflow\\python\\eager\\def_function.py\u001b[0m in \u001b[0;36m__call__\u001b[1;34m(self, *args, **kwds)\u001b[0m\n\u001b[0;32m    826\u001b[0m     \u001b[0mtracing_count\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mexperimental_get_tracing_count\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    827\u001b[0m     \u001b[1;32mwith\u001b[0m \u001b[0mtrace\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mTrace\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_name\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0mtm\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 828\u001b[1;33m       \u001b[0mresult\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_call\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    829\u001b[0m       \u001b[0mcompiler\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;34m\"xla\"\u001b[0m \u001b[1;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_experimental_compile\u001b[0m \u001b[1;32melse\u001b[0m \u001b[1;34m\"nonXla\"\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    830\u001b[0m       \u001b[0mnew_tracing_count\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mexperimental_get_tracing_count\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Roaming\\Python\\Python37\\site-packages\\tensorflow\\python\\eager\\def_function.py\u001b[0m in \u001b[0;36m_call\u001b[1;34m(self, *args, **kwds)\u001b[0m\n\u001b[0;32m    869\u001b[0m       \u001b[1;31m# This is the first call of __call__, so we have to initialize.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    870\u001b[0m       \u001b[0minitializers\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m[\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 871\u001b[1;33m       \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_initialize\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mkwds\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0madd_initializers_to\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0minitializers\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    872\u001b[0m     \u001b[1;32mfinally\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    873\u001b[0m       \u001b[1;31m# At this point we know that the initialization is complete (or less\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Roaming\\Python\\Python37\\site-packages\\tensorflow\\python\\eager\\def_function.py\u001b[0m in \u001b[0;36m_initialize\u001b[1;34m(self, args, kwds, add_initializers_to)\u001b[0m\n\u001b[0;32m    724\u001b[0m     self._concrete_stateful_fn = (\n\u001b[0;32m    725\u001b[0m         self._stateful_fn._get_concrete_function_internal_garbage_collected(  # pylint: disable=protected-access\n\u001b[1;32m--> 726\u001b[1;33m             *args, **kwds))\n\u001b[0m\u001b[0;32m    727\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    728\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0minvalid_creator_scope\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0munused_args\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0munused_kwds\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Roaming\\Python\\Python37\\site-packages\\tensorflow\\python\\eager\\function.py\u001b[0m in \u001b[0;36m_get_concrete_function_internal_garbage_collected\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   2967\u001b[0m       \u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mkwargs\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   2968\u001b[0m     \u001b[1;32mwith\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_lock\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 2969\u001b[1;33m       \u001b[0mgraph_function\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0m_\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_maybe_define_function\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   2970\u001b[0m     \u001b[1;32mreturn\u001b[0m \u001b[0mgraph_function\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   2971\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Roaming\\Python\\Python37\\site-packages\\tensorflow\\python\\eager\\function.py\u001b[0m in \u001b[0;36m_maybe_define_function\u001b[1;34m(self, args, kwargs)\u001b[0m\n\u001b[0;32m   3359\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   3360\u001b[0m           \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_function_cache\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmissed\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0madd\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mcall_context_key\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 3361\u001b[1;33m           \u001b[0mgraph_function\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_create_graph_function\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   3362\u001b[0m           \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_function_cache\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mprimary\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mcache_key\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mgraph_function\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   3363\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Roaming\\Python\\Python37\\site-packages\\tensorflow\\python\\eager\\function.py\u001b[0m in \u001b[0;36m_create_graph_function\u001b[1;34m(self, args, kwargs, override_flat_arg_shapes)\u001b[0m\n\u001b[0;32m   3204\u001b[0m             \u001b[0marg_names\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0marg_names\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   3205\u001b[0m             \u001b[0moverride_flat_arg_shapes\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0moverride_flat_arg_shapes\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 3206\u001b[1;33m             capture_by_value=self._capture_by_value),\n\u001b[0m\u001b[0;32m   3207\u001b[0m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_function_attributes\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   3208\u001b[0m         \u001b[0mfunction_spec\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfunction_spec\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Roaming\\Python\\Python37\\site-packages\\tensorflow\\python\\framework\\func_graph.py\u001b[0m in \u001b[0;36mfunc_graph_from_py_func\u001b[1;34m(name, python_func, args, kwargs, signature, func_graph, autograph, autograph_options, add_control_dependencies, arg_names, op_return_value, collections, capture_by_value, override_flat_arg_shapes)\u001b[0m\n\u001b[0;32m    988\u001b[0m         \u001b[0m_\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0moriginal_func\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtf_decorator\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0munwrap\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mpython_func\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    989\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 990\u001b[1;33m       \u001b[0mfunc_outputs\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mpython_func\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0mfunc_args\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mfunc_kwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    991\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    992\u001b[0m       \u001b[1;31m# invariant: `func_outputs` contains only Tensors, CompositeTensors,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Roaming\\Python\\Python37\\site-packages\\tensorflow\\python\\eager\\def_function.py\u001b[0m in \u001b[0;36mwrapped_fn\u001b[1;34m(*args, **kwds)\u001b[0m\n\u001b[0;32m    632\u001b[0m             \u001b[0mxla_context\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mExit\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    633\u001b[0m         \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 634\u001b[1;33m           \u001b[0mout\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mweak_wrapped_fn\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m__wrapped__\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    635\u001b[0m         \u001b[1;32mreturn\u001b[0m \u001b[0mout\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    636\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Roaming\\Python\\Python37\\site-packages\\tensorflow\\python\\framework\\func_graph.py\u001b[0m in \u001b[0;36mwrapper\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m    975\u001b[0m           \u001b[1;32mexcept\u001b[0m \u001b[0mException\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[1;33m:\u001b[0m  \u001b[1;31m# pylint:disable=broad-except\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    976\u001b[0m             \u001b[1;32mif\u001b[0m \u001b[0mhasattr\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0me\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m\"ag_error_metadata\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 977\u001b[1;33m               \u001b[1;32mraise\u001b[0m \u001b[0me\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mag_error_metadata\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mto_exception\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0me\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    978\u001b[0m             \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    979\u001b[0m               \u001b[1;32mraise\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mValueError\u001b[0m: in user code:\n\n    C:\\Users\\Pinky\\AppData\\Roaming\\Python\\Python37\\site-packages\\tensorflow\\python\\keras\\engine\\training.py:805 train_function  *\n        return step_function(self, iterator)\n    C:\\Users\\Pinky\\AppData\\Roaming\\Python\\Python37\\site-packages\\tensorflow\\python\\keras\\engine\\training.py:795 step_function  **\n        outputs = model.distribute_strategy.run(run_step, args=(data,))\n    C:\\Users\\Pinky\\AppData\\Roaming\\Python\\Python37\\site-packages\\tensorflow\\python\\distribute\\distribute_lib.py:1259 run\n        return self._extended.call_for_each_replica(fn, args=args, kwargs=kwargs)\n    C:\\Users\\Pinky\\AppData\\Roaming\\Python\\Python37\\site-packages\\tensorflow\\python\\distribute\\distribute_lib.py:2730 call_for_each_replica\n        return self._call_for_each_replica(fn, args, kwargs)\n    C:\\Users\\Pinky\\AppData\\Roaming\\Python\\Python37\\site-packages\\tensorflow\\python\\distribute\\distribute_lib.py:3417 _call_for_each_replica\n        return fn(*args, **kwargs)\n    C:\\Users\\Pinky\\AppData\\Roaming\\Python\\Python37\\site-packages\\tensorflow\\python\\keras\\engine\\training.py:788 run_step  **\n        outputs = model.train_step(data)\n    C:\\Users\\Pinky\\AppData\\Roaming\\Python\\Python37\\site-packages\\tensorflow\\python\\keras\\engine\\training.py:754 train_step\n        y_pred = self(x, training=True)\n    C:\\Users\\Pinky\\AppData\\Roaming\\Python\\Python37\\site-packages\\tensorflow\\python\\keras\\engine\\base_layer.py:1012 __call__\n        outputs = call_fn(inputs, *args, **kwargs)\n    C:\\Users\\Pinky\\AppData\\Roaming\\Python\\Python37\\site-packages\\tensorflow\\python\\keras\\engine\\sequential.py:375 call\n        return super(Sequential, self).call(inputs, training=training, mask=mask)\n    C:\\Users\\Pinky\\AppData\\Roaming\\Python\\Python37\\site-packages\\tensorflow\\python\\keras\\engine\\functional.py:425 call\n        inputs, training=training, mask=mask)\n    C:\\Users\\Pinky\\AppData\\Roaming\\Python\\Python37\\site-packages\\tensorflow\\python\\keras\\engine\\functional.py:560 _run_internal_graph\n        outputs = node.layer(*args, **kwargs)\n    C:\\Users\\Pinky\\AppData\\Roaming\\Python\\Python37\\site-packages\\tensorflow\\python\\keras\\layers\\recurrent.py:660 __call__\n        return super(RNN, self).__call__(inputs, **kwargs)\n    C:\\Users\\Pinky\\AppData\\Roaming\\Python\\Python37\\site-packages\\tensorflow\\python\\keras\\engine\\base_layer.py:998 __call__\n        input_spec.assert_input_compatibility(self.input_spec, inputs, self.name)\n    C:\\Users\\Pinky\\AppData\\Roaming\\Python\\Python37\\site-packages\\tensorflow\\python\\keras\\engine\\input_spec.py:223 assert_input_compatibility\n        str(tuple(shape)))\n\n    ValueError: Input 0 of layer lstm_6 is incompatible with the layer: expected ndim=3, found ndim=4. Full shape received: (None, 40, 38, 128)\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "\n",
    "vocab_size = 38\n",
    "embedding_dim = 128\n",
    "rnn_units = 4\n",
    "learning_rate = 0.001\n",
    "batch_size = 20\n",
    "num_epochs = 1000\n",
    "\n",
    "def build_model(vocab_size, embedding_dim, rnn_units):\n",
    "    model = tf.keras.Sequential([\n",
    "        tf.keras.layers.Embedding(vocab_size, embedding_dim),\n",
    "        tf.keras.layers.LSTM(rnn_units, return_sequences=True),\n",
    "        tf.keras.layers.Dense(vocab_size, activation='softmax')\n",
    "    ])\n",
    "    return model\n",
    "\n",
    "\n",
    "model = build_model(vocab_size, embedding_dim, rnn_units)\n",
    "\n",
    "\n",
    "optimizer = tf.keras.optimizers.Adam(learning_rate=learning_rate)\n",
    "loss_fn = tf.keras.losses.SparseCategoricalCrossentropy()\n",
    "model.compile(optimizer=optimizer, loss=loss_fn)\n",
    "\n",
    "history = model.fit(X, Y, batch_size=batch_size, epochs=num_epochs)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
