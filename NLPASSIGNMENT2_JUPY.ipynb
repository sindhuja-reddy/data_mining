{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "b61749a2",
   "metadata": {},
   "outputs": [],
   "source": [
    "import sklearn\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plot\n",
    "import seaborn as sns\n",
    "from sklearn.utils import resample"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "25d1c88a",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "#import plotly as px\n",
    "plt.style.use('fivethirtyeight')\n",
    "%matplotlib inline\n",
    "#from textblob import TextBlob\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "dfbc5abd",
   "metadata": {},
   "outputs": [],
   "source": [
    "df1 = pd.read_csv(r'C:/Users/Sindhuja_reddy/OneDrive/Desktop/Jupy/positive.review')\n",
    "df2 = pd.read_csv(r'C:/Users/Sindhuja_reddy/OneDrive/Desktop/Jupy/negative.review')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "c12fec67",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>holes:1 must:1 top_secret:1 he:1 center:1 other_civilans:1 the_pacific:1 the_navy:1 a_lot:1 surface_must:1 this_book:1 man_named:1 &lt;num&gt;_feet:2 would_strongly:1 put_down:1 norman_johnson:1 lawes:1 a_top:1 the_support:1 ten:1 on_random:1 typhoon:1 a_phycologist:1 pressure:1 actually_an:1 a_day:1 johnson_is:1 strange:1 civilans_to:1 explored:1 support:1 pacific_ocean:1 pressure_to:1 back_some:1 read:1 however_on:1 it_still:1 stuck:1 a_remote:1 american:1 find_out:1 not_have:1 other_crichton:1 analysis.i:1 black_holes:1 after_a:1 misssion:1 some_strange:1 half_mile:1 &lt;num&gt;_navy:1 actually:1 remote_location:1 the_story:1 behavior:1 civilans:2 research_that:1 michael_crichton:1 excellant_novel:1 strongly_recommend:1 some:2 michael:1 does_not:1 strange_things:1 sphere_by:1 a_man:1 are_joined:1 mile_long:1 put:1 operations:1 spacecraft:2 around:1 joined:1 an_american:1 sea:1 a_half:1 pacific:1 the_civilans:1 information_on:1 day_under:1 half:1 that_some:1 lot:1 around_a:1 travels:1 ten_is:1 excellant:1 however:1 novels:3 find:1 brought:1 random:1 one:1 the_ocean:3 team:1 a_typhoon:1 read_the:1 all_of:1 feet_under:2 by_michael:1 by_&lt;num&gt;:1 out_that:1 crichton_novels:3 support_ships:1 the_other:1 sea_they:1 strongly:1 has_brought:1 earth.this:1 he_travels:1 quickly:1 novels_have:1 things_back:1 ocean_is:1 personel:1 recommend:1 remote:1 leave:1 explored_black:1 a_center:1 run_operations:1 certainly_the:1 is_stuck:1 have_read:1 random_things:1 center_&lt;num&gt;:1 of_all:1 out:1 best:1 help:2 still:1 novel:2 they_are:1 the_lawes:1 feet:2 phycologist_he:1 day:1 down:1 phycologist:1 analysis.i_would:1 they_quickly:1 help_the:1 &lt;num&gt;_other:1 spaceship:1 hardest_to:1 lot_of:1 one_of:1 help_them:1 spacecraft_they:1 comes:1 back_to:1 holes_and:1 researching_the:1 lawes_of:1 crichton_is:1 stuck_&lt;num&gt;:1 to_help:2 crichton:4 to_earth.this:1 navy_in:1 behavior_analysis.i:1 researching:1 book:1 they:3 learn:1 operations_however:1 with_&lt;num&gt;:1 them:1 to_put:1 typhoon_comes:1 the_spacecraft:2 was:1 live_while:1 surface_of:1 partial:1 still_has:1 &lt;num&gt;:4 navy:2 spaceship_the:1 ships:1 the_crichton:1 named:1 norman:1 of_information:1 american_ship:1 man:1 spacecraft_is:1 top:1 ocean_after:1 revolves_around:1 all:1 other:2 of_partial:1 team_of:1 novels_that:1 must_leave:1 leave_the:1 live:1 i_have:1 the_team:1 mile:1 story:1 travels_with:1 location:1 earth.this_novel:1 novel_does:1 johnson:2 johnson_johnson:1 that_i:1 ship:1 some_of:1 surface:3 brought_back:1 them_run:1 while:1 of_ten:1 secret:1 long:1 has_explored:1 is_actually:1 to_live:1 was_certainly:1 the_surface:3 to_behavior:1 learn_that:1 named_norman:1 run:1 quickly_learn:1 this_was:1 i:1 the_hardest:1 the_best:1 ships_on:1 research:1 travel_to:1 long_spaceship:1 an_excellant:1 joined_by:1 secret_misssion:1 the_sea:1 certainly:1 personel_to:1 surface_a:1 while_researching:1 not:1 revolves:1 they_find:1 hardest:1 location_in:1 back:2 navy_personel:1 story_revolves:1 ocean:4 things_from:1 partial_pressure:1 best_crichton:1 comes_and:1 civilans_travel:1 misssion_they:1 recommend_this:1 sphere:1 the_research:1 black:1 down_of:1 things:2 travel:1 ocean_to:2 ship_that:1 after:1 information:1 novel_this:1 #label#:positive</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>i_think:1 dr_dean:1 reason:1 oz:2 medicine_whi...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>woman_the:1 contains_the:1 fan_i:1 alex_ross(s...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>hurricane:1 these_pages:1 lost_innocence:1 bot...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>while:1 commented:1 the_rise:2 if:2 strong_emp...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>the_unbuilt:1 must:1 a_\"large:1 me:1 brussels:...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  holes:1 must:1 top_secret:1 he:1 center:1 other_civilans:1 the_pacific:1 the_navy:1 a_lot:1 surface_must:1 this_book:1 man_named:1 <num>_feet:2 would_strongly:1 put_down:1 norman_johnson:1 lawes:1 a_top:1 the_support:1 ten:1 on_random:1 typhoon:1 a_phycologist:1 pressure:1 actually_an:1 a_day:1 johnson_is:1 strange:1 civilans_to:1 explored:1 support:1 pacific_ocean:1 pressure_to:1 back_some:1 read:1 however_on:1 it_still:1 stuck:1 a_remote:1 american:1 find_out:1 not_have:1 other_crichton:1 analysis.i:1 black_holes:1 after_a:1 misssion:1 some_strange:1 half_mile:1 <num>_navy:1 actually:1 remote_location:1 the_story:1 behavior:1 civilans:2 research_that:1 michael_crichton:1 excellant_novel:1 strongly_recommend:1 some:2 michael:1 does_not:1 strange_things:1 sphere_by:1 a_man:1 are_joined:1 mile_long:1 put:1 operations:1 spacecraft:2 around:1 joined:1 an_american:1 sea:1 a_half:1 pacific:1 the_civilans:1 information_on:1 day_under:1 half:1 that_some:1 lot:1 around_a:1 travels:1 ten_is:1 excellant:1 however:1 novels:3 find:1 brought:1 random:1 one:1 the_ocean:3 team:1 a_typhoon:1 read_the:1 all_of:1 feet_under:2 by_michael:1 by_<num>:1 out_that:1 crichton_novels:3 support_ships:1 the_other:1 sea_they:1 strongly:1 has_brought:1 earth.this:1 he_travels:1 quickly:1 novels_have:1 things_back:1 ocean_is:1 personel:1 recommend:1 remote:1 leave:1 explored_black:1 a_center:1 run_operations:1 certainly_the:1 is_stuck:1 have_read:1 random_things:1 center_<num>:1 of_all:1 out:1 best:1 help:2 still:1 novel:2 they_are:1 the_lawes:1 feet:2 phycologist_he:1 day:1 down:1 phycologist:1 analysis.i_would:1 they_quickly:1 help_the:1 <num>_other:1 spaceship:1 hardest_to:1 lot_of:1 one_of:1 help_them:1 spacecraft_they:1 comes:1 back_to:1 holes_and:1 researching_the:1 lawes_of:1 crichton_is:1 stuck_<num>:1 to_help:2 crichton:4 to_earth.this:1 navy_in:1 behavior_analysis.i:1 researching:1 book:1 they:3 learn:1 operations_however:1 with_<num>:1 them:1 to_put:1 typhoon_comes:1 the_spacecraft:2 was:1 live_while:1 surface_of:1 partial:1 still_has:1 <num>:4 navy:2 spaceship_the:1 ships:1 the_crichton:1 named:1 norman:1 of_information:1 american_ship:1 man:1 spacecraft_is:1 top:1 ocean_after:1 revolves_around:1 all:1 other:2 of_partial:1 team_of:1 novels_that:1 must_leave:1 leave_the:1 live:1 i_have:1 the_team:1 mile:1 story:1 travels_with:1 location:1 earth.this_novel:1 novel_does:1 johnson:2 johnson_johnson:1 that_i:1 ship:1 some_of:1 surface:3 brought_back:1 them_run:1 while:1 of_ten:1 secret:1 long:1 has_explored:1 is_actually:1 to_live:1 was_certainly:1 the_surface:3 to_behavior:1 learn_that:1 named_norman:1 run:1 quickly_learn:1 this_was:1 i:1 the_hardest:1 the_best:1 ships_on:1 research:1 travel_to:1 long_spaceship:1 an_excellant:1 joined_by:1 secret_misssion:1 the_sea:1 certainly:1 personel_to:1 surface_a:1 while_researching:1 not:1 revolves:1 they_find:1 hardest:1 location_in:1 back:2 navy_personel:1 story_revolves:1 ocean:4 things_from:1 partial_pressure:1 best_crichton:1 comes_and:1 civilans_travel:1 misssion_they:1 recommend_this:1 sphere:1 the_research:1 black:1 down_of:1 things:2 travel:1 ocean_to:2 ship_that:1 after:1 information:1 novel_this:1 #label#:positive\n",
       "0  i_think:1 dr_dean:1 reason:1 oz:2 medicine_whi...                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                              \n",
       "1  woman_the:1 contains_the:1 fan_i:1 alex_ross(s...                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                              \n",
       "2  hurricane:1 these_pages:1 lost_innocence:1 bot...                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                              \n",
       "3  while:1 commented:1 the_rise:2 if:2 strong_emp...                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                              \n",
       "4  the_unbuilt:1 must:1 a_\"large:1 me:1 brussels:...                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                              "
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df1.head()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "cfcf7e45",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>avid:1 your:1 horrible_book:1 wasted:1 use_it:1 the_entire:1 money.i:1 i_lit:1 i_read:1 lit:1 i_would:1 relationship:1 read:1 a_&lt;num&gt;:1 reader_and:1 reader:1 suffering:1 fire_one:1 i_had:1 year_old:2 gotten:1 horrible:3 lit_this:1 world...don't:1 my:2 one_star:1 headache_the:1 this_book:5 mom:1 was_horrible:1 friend:1 book_horrible:1 star_i:1 back:1 avid_reader:1 than_one:1 life:1 copy:1 rate_it:1 rate:1 my_mom:1 man:1 book_was:1 half:1 on_fire:1 and_then:1 reading_this:1 so:1 lower:1 i_could:1 &lt;num&gt;_year:2 than:1 time:2 half_of:1 time_spent:1 then:1 book:6 and_picked:1 possible:1 spent:1 old_man:1 up_after:1 one:2 horrible_if:1 one_less:1 part:1 was:2 entire:1 less_copy:1 to_rate:1 my_life:1 about_the:1 your_money.i:1 an_avid:1 if:1 the_relationship:1 use:1 a_headache:1 fire:1 lower_than:1 reading:1 a_friend:1 picked:1 purposes:1 then_got:1 waste_your:1 after_my:1 friend_i:1 old:2 man_and:1 and_i:1 world...don't_waste:1 book_on:1 part_about:1 copy_in:1 book_back:1 book_wasted:1 have_i:1 time_and:1 the_world...don't:1 better:1 if_it:1 star:1 got:1 mom_had:1 read_half:1 waste:1 after:1 i:6 about:1 could_use:1 had_gotten:1 was_possible:1 year:2 it_lower:1 relationship_the:1 wasted_my:1 wish:1 wish_i:1 boy:1 purposes_this:1 got_to:1 the_time:1 it_was:1 back_so:1 suffering_from:1 spent_reading:1 book_up:1 less:1 better_purposes:1 headache:1 possible_to:1 money.i_wish:1 for_better:1 it_suffering:1 the_part:1 gotten_it:1 picked_this:1 entire_time:1 old_boy:1 i_am:1 the_&lt;num&gt;:1 boy_had:1 &lt;num&gt;:2 so_i:1 #label#:negative</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>to_use:1 shallow:1 found:1 he_castigates:1 cas...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>avid:1 your:1 horrible_book:1 wasted:1 use_it:...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>book_seriously:1 we:1 days_couldn't:1 me_tell:...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>mass:1 only:1 he:2 help:1 \"jurisfiction\":1 lik...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>save_your:1 class_and:1 his_facts:1 opinions:1...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  avid:1 your:1 horrible_book:1 wasted:1 use_it:1 the_entire:1 money.i:1 i_lit:1 i_read:1 lit:1 i_would:1 relationship:1 read:1 a_<num>:1 reader_and:1 reader:1 suffering:1 fire_one:1 i_had:1 year_old:2 gotten:1 horrible:3 lit_this:1 world...don't:1 my:2 one_star:1 headache_the:1 this_book:5 mom:1 was_horrible:1 friend:1 book_horrible:1 star_i:1 back:1 avid_reader:1 than_one:1 life:1 copy:1 rate_it:1 rate:1 my_mom:1 man:1 book_was:1 half:1 on_fire:1 and_then:1 reading_this:1 so:1 lower:1 i_could:1 <num>_year:2 than:1 time:2 half_of:1 time_spent:1 then:1 book:6 and_picked:1 possible:1 spent:1 old_man:1 up_after:1 one:2 horrible_if:1 one_less:1 part:1 was:2 entire:1 less_copy:1 to_rate:1 my_life:1 about_the:1 your_money.i:1 an_avid:1 if:1 the_relationship:1 use:1 a_headache:1 fire:1 lower_than:1 reading:1 a_friend:1 picked:1 purposes:1 then_got:1 waste_your:1 after_my:1 friend_i:1 old:2 man_and:1 and_i:1 world...don't_waste:1 book_on:1 part_about:1 copy_in:1 book_back:1 book_wasted:1 have_i:1 time_and:1 the_world...don't:1 better:1 if_it:1 star:1 got:1 mom_had:1 read_half:1 waste:1 after:1 i:6 about:1 could_use:1 had_gotten:1 was_possible:1 year:2 it_lower:1 relationship_the:1 wasted_my:1 wish:1 wish_i:1 boy:1 purposes_this:1 got_to:1 the_time:1 it_was:1 back_so:1 suffering_from:1 spent_reading:1 book_up:1 less:1 better_purposes:1 headache:1 possible_to:1 money.i_wish:1 for_better:1 it_suffering:1 the_part:1 gotten_it:1 picked_this:1 entire_time:1 old_boy:1 i_am:1 the_<num>:1 boy_had:1 <num>:2 so_i:1 #label#:negative\n",
       "0  to_use:1 shallow:1 found:1 he_castigates:1 cas...                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                               \n",
       "1  avid:1 your:1 horrible_book:1 wasted:1 use_it:...                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                               \n",
       "2  book_seriously:1 we:1 days_couldn't:1 me_tell:...                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                               \n",
       "3  mass:1 only:1 he:2 help:1 \"jurisfiction\":1 lik...                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                               \n",
       "4  save_your:1 class_and:1 his_facts:1 opinions:1...                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                               "
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df2.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "ffc15003",
   "metadata": {},
   "outputs": [],
   "source": [
    "class SentimentCorpus:\n",
    "    \n",
    "    def _init_(self, train_per=0.8, dev_per=0, test_per=0.2):\n",
    "        '''\n",
    "        prepare dataset\n",
    "        1) build feature dictionaries\n",
    "        2) split data into train/dev/test sets \n",
    "        '''\n",
    "        X, y, feat_dict, feat_counts = build_dicts()\n",
    "        self.nr_instances = y.shape[0]\n",
    "        self.nr_features = X.shape[1]\n",
    "        self.X = X\n",
    "        self.y = y\n",
    "        self.feat_dict = feat_dict\n",
    "        self.feat_counts = feat_counts\n",
    "        \n",
    "        train_y, dev_y, test_y, train_X, dev_X, test_X = split_train_dev_test(self.X, self.y, train_per, dev_per, test_per)\n",
    "        self.train_X = train_X\n",
    "        self.train_y = train_y\n",
    "        self.dev_X = dev_X\n",
    "        self.dev_y = dev_y\n",
    "        self.test_X = test_X\n",
    "        self.test_y = test_y\n",
    "\n",
    "def split_train_dev_test(X, y, train_per, dev_per, test_per):\n",
    "    if (train_per + dev_per + test_per) > 1:\n",
    "       \n",
    "        print (\"train/dev/test splits should sum to one\")\n",
    "        return\n",
    "    dim = y.shape[0]\n",
    "    split1 = int(dim * train_per)\n",
    "    \n",
    "    if dev_per == 0:\n",
    "        train_y, test_y = np.vsplit(y, [split1])\n",
    "        dev_y = np.array([])\n",
    "        train_X = X[0:split1,:]\n",
    "        test_X = X[split1:,:]\n",
    "        dev_X = np.array([])\n",
    "    else:\n",
    "        split2 = int(dim*(train_per+dev_per))\n",
    "        train_y,dev_y,test_y = np.vsplit(y,(split1,split2))\n",
    "        train_X = X[0:split1,:]\n",
    "        dev_X = X[split1:split2,:]\n",
    "        test_X = X[split2:,:]\n",
    "        \n",
    "    return train_y,dev_y,test_y,train_X,dev_X,test_X\n",
    "\n",
    "def build_dicts():\n",
    "    '''\n",
    "    builds feature dictionaries\n",
    "    ''' \n",
    "    feat_counts = {}\n",
    "\n",
    "    # build feature dictionary with counts\n",
    "    nr_pos = 0\n",
    "    with codecs.open(\"positive.review\", 'r', 'utf8') as pos_file:\n",
    "        for line in pos_file:\n",
    "            nr_pos += 1\n",
    "            toks = line.split(\" \")\n",
    "            for feat in toks[0:-1]:\n",
    "                name, counts = feat.split(\":\")\n",
    "                if name not in feat_counts:\n",
    "                    feat_counts[name] = 0\n",
    "                feat_counts[name] += int(counts)\n",
    "    \n",
    "    nr_neg = 0\n",
    "    with codecs.open(\"negative.review\", 'r', 'utf8') as neg_file:\n",
    "        for line in neg_file:\n",
    "            nr_neg += 1\n",
    "            toks = line.split(\" \")\n",
    "            for feat in toks[0:-1]:\n",
    "                name, counts = feat.split(\":\")\n",
    "                if name not in feat_counts:\n",
    "                    feat_counts[name] = 0\n",
    "                feat_counts[name] += int(counts)\n",
    "\n",
    "    # remove all features that occur less than 5 (threshold) times\n",
    "    to_remove = []\n",
    "    #changed by sirsh: iteritems -> items\n",
    "    for key, value in feat_counts.items():\n",
    "        if value < 5:\n",
    "            to_remove.append(key)\n",
    "    for key in to_remove:\n",
    "        del feat_counts[key]\n",
    "\n",
    "    # map feature to index\n",
    "    feat_dict = {}\n",
    "    i = 0\n",
    "    for key in feat_counts.keys():\n",
    "        feat_dict[key] = i\n",
    "        i += 1\n",
    "\n",
    "    nr_feat = len(feat_counts) \n",
    "    nr_instances = nr_pos + nr_neg\n",
    "    X = np.zeros((nr_instances, nr_feat), dtype=float)\n",
    "    y = np.vstack((np.zeros([nr_pos,1], dtype=int), np.ones([nr_neg,1], dtype=int)))\n",
    "    \n",
    "    with codecs.open(\"positive.review\", 'r', 'utf8') as pos_file:\n",
    "        nr_pos = 0\n",
    "        for line in pos_file:\n",
    "            toks = line.split(\" \")\n",
    "            for feat in toks[0:-1]:\n",
    "                name, counts = feat.split(\":\")\n",
    "                if name in feat_dict:\n",
    "                    X[nr_pos,feat_dict[name]] = int(counts)\n",
    "            nr_pos += 1\n",
    "        \n",
    "    with codecs.open(\"negative.review\", 'r', 'utf8') as neg_file:\n",
    "        nr_neg = 0\n",
    "        for line in neg_file:\n",
    "            toks = line.split(\" \")\n",
    "            for feat in toks[0:-1]:\n",
    "                name, counts = feat.split(\":\")\n",
    "                if name in feat_dict:\n",
    "                    X[nr_pos+nr_neg,feat_dict[name]] = int(counts)\n",
    "            nr_neg += 1\n",
    "    \n",
    "    # shuffle the order, mix positive and negative examples\n",
    "    new_order = np.arange(nr_instances)\n",
    "    np.random.seed(0) # set seed\n",
    "    np.random.shuffle(new_order)\n",
    "    X = X[new_order,:]\n",
    "    y = y[new_order,:]\n",
    "    \n",
    "    return X, y, feat_dict, feat_counts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "f35390e5",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "af9657bd",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "8ef45074",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "holes:1 must:1 top_secret:1 he:1 center:1 other_civilans:1 the_pacific:1 the_navy:1 a_lot:1 surface_must:1 this_book:1 man_named:1 <num>_feet:2 would_strongly:1 put_down:1 norman_johnson:1 lawes:1 a_top:1 the_support:1 ten:1 on_random:1 typhoon:1 a_phycologist:1 pressure:1 actually_an:1 a_day:1 johnson_is:1 strange:1 civilans_to:1 explored:1 support:1 pacific_ocean:1 pressure_to:1 back_some:1 read:1 however_on:1 it_still:1 stuck:1 a_remote:1 american:1 find_out:1 not_have:1 other_crichton:1 analysis.i:1 black_holes:1 after_a:1 misssion:1 some_strange:1 half_mile:1 <num>_navy:1 actually:1 remote_location:1 the_story:1 behavior:1 civilans:2 research_that:1 michael_crichton:1 excellant_novel:1 strongly_recommend:1 some:2 michael:1 does_not:1 strange_things:1 sphere_by:1 a_man:1 are_joined:1 mile_long:1 put:1 operations:1 spacecraft:2 around:1 joined:1 an_american:1 sea:1 a_half:1 pacific:1 the_civilans:1 information_on:1 day_under:1 half:1 that_some:1 lot:1 around_a:1 travels:1 ten_is:1 excellant:1 however:1 novels:3 find:1 brought:1 random:1 one:1 the_ocean:3 team:1 a_typhoon:1 read_the:1 all_of:1 feet_under:2 by_michael:1 by_<num>:1 out_that:1 crichton_novels:3 support_ships:1 the_other:1 sea_they:1 strongly:1 has_brought:1 earth.this:1 he_travels:1 quickly:1 novels_have:1 things_back:1 ocean_is:1 personel:1 recommend:1 remote:1 leave:1 explored_black:1 a_center:1 run_operations:1 certainly_the:1 is_stuck:1 have_read:1 random_things:1 center_<num>:1 of_all:1 out:1 best:1 help:2 still:1 novel:2 they_are:1 the_lawes:1 feet:2 phycologist_he:1 day:1 down:1 phycologist:1 analysis.i_would:1 they_quickly:1 help_the:1 <num>_other:1 spaceship:1 hardest_to:1 lot_of:1 one_of:1 help_them:1 spacecraft_they:1 comes:1 back_to:1 holes_and:1 researching_the:1 lawes_of:1 crichton_is:1 stuck_<num>:1 to_help:2 crichton:4 to_earth.this:1 navy_in:1 behavior_analysis.i:1 researching:1 book:1 they:3 learn:1 operations_however:1 with_<num>:1 them:1 to_put:1 typhoon_comes:1 the_spacecraft:2 was:1 live_while:1 surface_of:1 partial:1 still_has:1 <num>:4 navy:2 spaceship_the:1 ships:1 the_crichton:1 named:1 norman:1 of_information:1 american_ship:1 man:1 spacecraft_is:1 top:1 ocean_after:1 revolves_around:1 all:1 other:2 of_partial:1 team_of:1 novels_that:1 must_leave:1 leave_the:1 live:1 i_have:1 the_team:1 mile:1 story:1 travels_with:1 location:1 earth.this_novel:1 novel_does:1 johnson:2 johnson_johnson:1 that_i:1 ship:1 some_of:1 surface:3 brought_back:1 them_run:1 while:1 of_ten:1 secret:1 long:1 has_explored:1 is_actually:1 to_live:1 was_certainly:1 the_surface:3 to_behavior:1 learn_that:1 named_norman:1 run:1 quickly_learn:1 this_was:1 i:1 the_hardest:1 the_best:1 ships_on:1 research:1 travel_to:1 long_spaceship:1 an_excellant:1 joined_by:1 secret_misssion:1 the_sea:1 certainly:1 personel_to:1 surface_a:1 while_researching:1 not:1 revolves:1 they_find:1 hardest:1 location_in:1 back:2 navy_personel:1 story_revolves:1 ocean:4 things_from:1 partial_pressure:1 best_crichton:1 comes_and:1 civilans_travel:1 misssion_they:1 recommend_this:1 sphere:1 the_research:1 black:1 down_of:1 things:2 travel:1 ocean_to:2 ship_that:1 after:1 information:1 novel_this:1 #label#:positive    0\n",
       "dtype: int64"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "e2dc19e7",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.decomposition import NMF\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "da18789e",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'train_X' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn [33], line 12\u001b[0m\n\u001b[0;32m     10\u001b[0m hidden_layers \u001b[38;5;241m=\u001b[39m hidden_unit\u001b[38;5;241m*\u001b[39ml\n\u001b[0;32m     11\u001b[0m model \u001b[38;5;241m=\u001b[39m MLPClassifier(hidden_layer_sizes \u001b[38;5;241m=\u001b[39m hidden_layers, activation \u001b[38;5;241m=\u001b[39m a,)\n\u001b[1;32m---> 12\u001b[0m model\u001b[38;5;241m.\u001b[39mfit(\u001b[43mtrain_X\u001b[49m,y_train)\n\u001b[0;32m     13\u001b[0m y_pred \u001b[38;5;241m=\u001b[39m model\u001b[38;5;241m.\u001b[39mpredict(x_test)\n\u001b[0;32m     14\u001b[0m acc \u001b[38;5;241m=\u001b[39m accuracy_score(y_test,y_pred)\n",
      "\u001b[1;31mNameError\u001b[0m: name 'train_X' is not defined"
     ]
    }
   ],
   "source": [
    "from sklearn.neural_network import MLPClassifier\n",
    "from sklearn.metrics import precision_score, recall_score, f1_score, accuracy_score\n",
    "\n",
    "layers =[1,2,3]\n",
    "activation = [\"logistic\",\"TanH\",\"RelU\"]\n",
    "hidden_unit = 100\n",
    "\n",
    "for l in layers:\n",
    "    for a in activation:\n",
    "        hidden_layers = hidden_unit*l\n",
    "        model = MLPClassifier(hidden_layer_sizes = hidden_layers, activation = a,)\n",
    "        model.fit(train_X,y_train)\n",
    "        y_pred = model.predict(x_test)\n",
    "        acc = accuracy_score(y_test,y_pred)\n",
    "        rec = recall_score(y_test,y_pred)\n",
    "        prec = precision_score(y_test,y_pred)\n",
    "        f1 = f1_score(y_test,y_pred)\n",
    "        print(\"layers:\",l)\n",
    "        print(\"activation:\",a)\n",
    "        print(\"Accuracy:\",acc)\n",
    "        print(\"precision:\",prec)\n",
    "        print(\"recall:\",rec)\n",
    "        print(\"f1 score:\",f1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "43677e54",
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "'SentimentCorpus' object has no attribute 'train_X'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "Cell \u001b[1;32mIn [32], line 3\u001b[0m\n\u001b[0;32m      1\u001b[0m cor \u001b[38;5;241m=\u001b[39m SentimentCorpus()\n\u001b[1;32m----> 3\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;28mlen\u001b[39m(\u001b[43mcor\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtrain_X\u001b[49m))\n\u001b[0;32m      4\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;28mlen\u001b[39m(cor\u001b[38;5;241m.\u001b[39mtest_y))\n\u001b[0;32m      6\u001b[0m \u001b[38;5;28mprint\u001b[39m(cor\u001b[38;5;241m.\u001b[39mtrain_X\u001b[38;5;241m.\u001b[39mshape)\n",
      "\u001b[1;31mAttributeError\u001b[0m: 'SentimentCorpus' object has no attribute 'train_X'"
     ]
    }
   ],
   "source": [
    "cor = SentimentCorpus()\n",
    "\n",
    "print(len(cor.train_X))\n",
    "print(len(cor.test_y))\n",
    "\n",
    "print(cor.train_X.shape)\n",
    "\n",
    "print(cor.train_y.ravel())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "86e53275",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'cor' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn [31], line 5\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01msklearn\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mneural_network\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m MLPClassifier\n\u001b[0;32m      3\u001b[0m c \u001b[38;5;241m=\u001b[39m MLPClassifier(hidden_layer_sizes\u001b[38;5;241m=\u001b[39m(\u001b[38;5;241m100\u001b[39m,),activation\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mlogistic\u001b[39m\u001b[38;5;124m\"\u001b[39m,random_state\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m0\u001b[39m)\n\u001b[1;32m----> 5\u001b[0m c\u001b[38;5;241m.\u001b[39mfit(\u001b[43mcor\u001b[49m\u001b[38;5;241m.\u001b[39mtrain_X, cor\u001b[38;5;241m.\u001b[39mtrain_y\u001b[38;5;241m.\u001b[39mravel())\n",
      "\u001b[1;31mNameError\u001b[0m: name 'cor' is not defined"
     ]
    }
   ],
   "source": [
    "from sklearn.neural_network import MLPClassifier\n",
    "\n",
    "c = MLPClassifier(hidden_layer_sizes=(100,),activation=\"logistic\",random_state=0)\n",
    "\n",
    "c.fit(cor.train_X, cor.train_y.ravel())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "a51315ed",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "avid:1 your:1 horrible_book:1 wasted:1 use_it:1 the_entire:1 money.i:1 i_lit:1 i_read:1 lit:1 i_would:1 relationship:1 read:1 a_<num>:1 reader_and:1 reader:1 suffering:1 fire_one:1 i_had:1 year_old:2 gotten:1 horrible:3 lit_this:1 world...don't:1 my:2 one_star:1 headache_the:1 this_book:5 mom:1 was_horrible:1 friend:1 book_horrible:1 star_i:1 back:1 avid_reader:1 than_one:1 life:1 copy:1 rate_it:1 rate:1 my_mom:1 man:1 book_was:1 half:1 on_fire:1 and_then:1 reading_this:1 so:1 lower:1 i_could:1 <num>_year:2 than:1 time:2 half_of:1 time_spent:1 then:1 book:6 and_picked:1 possible:1 spent:1 old_man:1 up_after:1 one:2 horrible_if:1 one_less:1 part:1 was:2 entire:1 less_copy:1 to_rate:1 my_life:1 about_the:1 your_money.i:1 an_avid:1 if:1 the_relationship:1 use:1 a_headache:1 fire:1 lower_than:1 reading:1 a_friend:1 picked:1 purposes:1 then_got:1 waste_your:1 after_my:1 friend_i:1 old:2 man_and:1 and_i:1 world...don't_waste:1 book_on:1 part_about:1 copy_in:1 book_back:1 book_wasted:1 have_i:1 time_and:1 the_world...don't:1 better:1 if_it:1 star:1 got:1 mom_had:1 read_half:1 waste:1 after:1 i:6 about:1 could_use:1 had_gotten:1 was_possible:1 year:2 it_lower:1 relationship_the:1 wasted_my:1 wish:1 wish_i:1 boy:1 purposes_this:1 got_to:1 the_time:1 it_was:1 back_so:1 suffering_from:1 spent_reading:1 book_up:1 less:1 better_purposes:1 headache:1 possible_to:1 money.i_wish:1 for_better:1 it_suffering:1 the_part:1 gotten_it:1 picked_this:1 entire_time:1 old_boy:1 i_am:1 the_<num>:1 boy_had:1 <num>:2 so_i:1 #label#:negative    0\n",
       "dtype: int64"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df2.isnull().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "177cdd38",
   "metadata": {},
   "outputs": [
    {
     "ename": "FileNotFoundError",
     "evalue": "[Errno 2] No such file or directory: '/content/glove.6B.300d.txt'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn [34], line 7\u001b[0m\n\u001b[0;32m      3\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mnumpy\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m \u001b[38;5;21;01mnp\u001b[39;00m\n\u001b[0;32m      5\u001b[0m glove_vecs \u001b[38;5;241m=\u001b[39m {}\n\u001b[1;32m----> 7\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m \u001b[38;5;28;43mopen\u001b[39;49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43m/content/glove.6B.300d.txt\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mencoding\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mutf8\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m \u001b[38;5;28;01mas\u001b[39;00m f:\n\u001b[0;32m      8\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m line \u001b[38;5;129;01min\u001b[39;00m f:\n\u001b[0;32m      9\u001b[0m         word, vec \u001b[38;5;241m=\u001b[39m line\u001b[38;5;241m.\u001b[39msplit(maxsplit\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1\u001b[39m)\n",
      "\u001b[1;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: '/content/glove.6B.300d.txt'"
     ]
    }
   ],
   "source": [
    "glove_vecs = {}\n",
    "\n",
    "import numpy as np\n",
    "\n",
    "glove_vecs = {}\n",
    "\n",
    "with open(\"/content/glove.6B.300d.txt\", encoding=\"utf8\") as f:\n",
    "    for line in f:\n",
    "        word, vec = line.split(maxsplit=1)\n",
    "        glove_vecs[word] = np.fromstring(vec, sep=\" \")\n",
    "\n",
    "corpus_words = set(sent_corp.feat_dict.keys())\n",
    "glove_words = set(glove_vecs.keys())\n",
    "\n",
    "words_not_present = list(corpus_words - glove_words)\n",
    "\n",
    "for word in sent_corp.feat_dict.keys():\n",
    "    if word not in glove_vecs.keys():\n",
    "        sub_words = word.split(\"_\")\n",
    "        for sub_word in sub_words:\n",
    "            sub_word = sub_word.replace(\"'\", \"\") # Remove apostrophes\n",
    "            if sub_word not in glove_vecs.keys():\n",
    "                print(sub_word)\n",
    "\n",
    "embedding_dict = {}\n",
    "\n",
    "for word in sent_corp.feat_dict.keys():\n",
    "    if word in glove_vecs:\n",
    "        embedding_dict[word] = glove_vecs[word]\n",
    "    else:\n",
    "        emb = np.zeros(300)\n",
    "        sub_words = word.split(\"_\")\n",
    "        emb_vecs = []\n",
    "        for sub_word in sub_words:\n",
    "            sub_word = sub_word.replace(\"'\", \"\") # Remove apostrophes\n",
    "            if sub_word in glove_vecs:\n",
    "                emb_vecs.append(glove_vecs[sub_word])\n",
    "        if len(emb_vecs) > 0:\n",
    "            emb_vecs = np.array(emb_vecs)\n",
    "            emb = np.mean(emb_vecs, axis=0)\n",
    "        embedding_dict[word] = emb\n",
    "\n",
    "def convert_data_embedding(data):\n",
    "    emb_data = []\n",
    "    for datum in data:\n",
    "        count = 0\n",
    "        sent_emb_list = []\n",
    "        for word in sent_corp.feat_dict:\n",
    "            if word in embedding_dict:\n",
    "                sent_emb_list.append(datum[sent_corp.feat_dict[word]]*embedding_dict[word])\n",
    "                count += datum[sent_corp.feat_dict[word]]\n",
    "        if count > 0:\n",
    "            sent_emb_list = np.array(sent_emb_list)\n",
    "            final_emb = np.einsum(\"ij->j\",sent_emb_list)\n",
    "            final_emb = final_emb / count\n",
    "        else:\n",
    "            final_emb = np.zeros(50)\n",
    "        emb_data.append(final_emb)\n",
    "    emb_data = np.array(emb_data)\n",
    "    return emb_data\n",
    "\n",
    "train_x_emb = convert_data_embedding(sent_corp.train_X)\n",
    "test_x_emb = convert_data_embedding(sent_corp.test_X)\n",
    "\n",
    "def best_setup_experiment(num_layers=3, activation_f=\"logistic\"):\n",
    "    clf = MLPClassifier(hidden_layer_sizes=(100,)*num_layers,activation=activation_f, random_state=0, solver= 'sgd')\n",
    "    clf.fit(train_x_emb, sent_corp.train_y.ravel())\n",
    "    predictions = clf.predict(test_x_emb)\n",
    "    acc_score = accuracy_score(sent_corp.test_y.ravel(),predictions)\n",
    "    pres_score = precision_score(sent_corp.test_y.ravel(),predictions)\n",
    "    rec_score = recall_score(sent_corp.test_y.ravel(),predictions)\n",
    "    f_score = f1_score(sent_corp.test_y.ravel(),predictions)\n",
    "    return acc_score,pres_score,rec_score,f_score\n",
    "\n",
    "a_score, p_score, r_score, f_score = best_setup_experiment()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "49aa2249",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'x_train' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn [11], line 12\u001b[0m\n\u001b[0;32m     10\u001b[0m hidden_layers \u001b[38;5;241m=\u001b[39m hidden_unit\u001b[38;5;241m*\u001b[39ml\n\u001b[0;32m     11\u001b[0m model \u001b[38;5;241m=\u001b[39m MLPClassifier(hidden_layer_sizes \u001b[38;5;241m=\u001b[39m hidden_layers, activation \u001b[38;5;241m=\u001b[39m a,)\n\u001b[1;32m---> 12\u001b[0m model\u001b[38;5;241m.\u001b[39mfit(\u001b[43mx_train\u001b[49m,y_train)\n\u001b[0;32m     13\u001b[0m y_pred \u001b[38;5;241m=\u001b[39m model\u001b[38;5;241m.\u001b[39mpredict(x_test)\n\u001b[0;32m     14\u001b[0m acc \u001b[38;5;241m=\u001b[39m accuracy_score(y_test,y_pred)\n",
      "\u001b[1;31mNameError\u001b[0m: name 'x_train' is not defined"
     ]
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "5724471c",
   "metadata": {},
   "outputs": [
    {
     "ename": "FileNotFoundError",
     "evalue": "[Errno 2] No such file or directory: '/content/glove.6B.300d.txt'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn [21], line 7\u001b[0m\n\u001b[0;32m      3\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mnumpy\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m \u001b[38;5;21;01mnp\u001b[39;00m\n\u001b[0;32m      5\u001b[0m glove_vecs \u001b[38;5;241m=\u001b[39m {}\n\u001b[1;32m----> 7\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m \u001b[38;5;28;43mopen\u001b[39;49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43m/content/glove.6B.300d.txt\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mencoding\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mutf8\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m \u001b[38;5;28;01mas\u001b[39;00m f:\n\u001b[0;32m      8\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m line \u001b[38;5;129;01min\u001b[39;00m f:\n\u001b[0;32m      9\u001b[0m         word, vec \u001b[38;5;241m=\u001b[39m line\u001b[38;5;241m.\u001b[39msplit(maxsplit\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1\u001b[39m)\n",
      "\u001b[1;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: '/content/glove.6B.300d.txt'"
     ]
    }
   ],
   "source": [
    "glove_vecs = {}\n",
    "\n",
    "import numpy as np\n",
    "\n",
    "glove_vecs = {}\n",
    "\n",
    "with open(\"/content/glove.6B.300d.txt\", encoding=\"utf8\") as f:\n",
    "    for line in f:\n",
    "        word, vec = line.split(maxsplit=1)\n",
    "        glove_vecs[word] = np.fromstring(vec, sep=\" \")\n",
    "\n",
    "corpus_words = set(sent_corp.feat_dict.keys())\n",
    "glove_words = set(glove_vecs.keys())\n",
    "\n",
    "words_not_present = list(corpus_words - glove_words)\n",
    "\n",
    "for word in sent_corp.feat_dict.keys():\n",
    "    if word not in glove_vecs.keys():\n",
    "        sub_words = word.split(\"_\")\n",
    "        for sub_word in sub_words:\n",
    "            sub_word = sub_word.replace(\"'\", \"\") # Remove apostrophes\n",
    "            if sub_word not in glove_vecs.keys():\n",
    "                print(sub_word)\n",
    "\n",
    "embedding_dict = {}\n",
    "\n",
    "for word in sent_corp.feat_dict.keys():\n",
    "    if word in glove_vecs:\n",
    "        embedding_dict[word] = glove_vecs[word]\n",
    "    else:\n",
    "        emb = np.zeros(300)\n",
    "        sub_words = word.split(\"_\")\n",
    "        emb_vecs = []\n",
    "        for sub_word in sub_words:\n",
    "            sub_word = sub_word.replace(\"'\", \"\") # Remove apostrophes\n",
    "            if sub_word in glove_vecs:\n",
    "                emb_vecs.append(glove_vecs[sub_word])\n",
    "        if len(emb_vecs) > 0:\n",
    "            emb_vecs = np.array(emb_vecs)\n",
    "            emb = np.mean(emb_vecs, axis=0)\n",
    "        embedding_dict[word] = emb\n",
    "\n",
    "def convert_data_embedding(data):\n",
    "    emb_data = []\n",
    "    for datum in data:\n",
    "        count = 0\n",
    "        sent_emb_list = []\n",
    "        for word in sent_corp.feat_dict:\n",
    "            if word in embedding_dict:\n",
    "                sent_emb_list.append(datum[sent_corp.feat_dict[word]]*embedding_dict[word])\n",
    "                count += datum[sent_corp.feat_dict[word]]\n",
    "        if count > 0:\n",
    "            sent_emb_list = np.array(sent_emb_list)\n",
    "            final_emb = np.einsum(\"ij->j\",sent_emb_list)\n",
    "            final_emb = final_emb / count\n",
    "        else:\n",
    "            final_emb = np.zeros(50)\n",
    "        emb_data.append(final_emb)\n",
    "    emb_data = np.array(emb_data)\n",
    "    return emb_data\n",
    "\n",
    "train_x_emb = convert_data_embedding(sent_corp.train_X)\n",
    "test_x_emb = convert_data_embedding(sent_corp.test_X)\n",
    "\n",
    "def best_setup_experiment(num_layers=3, activation_f=\"logistic\"):\n",
    "    clf = MLPClassifier(hidden_layer_sizes=(100,)*num_layers,activation=activation_f, random_state=0, solver= 'sgd')\n",
    "    clf.fit(train_x_emb, sent_corp.train_y.ravel())\n",
    "    predictions = clf.predict(test_x_emb)\n",
    "    acc_score = accuracy_score(sent_corp.test_y.ravel(),predictions)\n",
    "    pres_score = precision_score(sent_corp.test_y.ravel(),predictions)\n",
    "    rec_score = recall_score(sent_corp.test_y.ravel(),predictions)\n",
    "    f_score = f1_score(sent_corp.test_y.ravel(),predictions)\n",
    "    return acc_score,pres_score,rec_score,f_score\n",
    "\n",
    "a_score, p_score, r_score, f_score = best_setup_experiment()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "894ce3a8",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
